{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lyricsgenius\n",
      "  Using cached lyricsgenius-3.0.1-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\programdata\\anaconda3\\envs\\melody\\lib\\site-packages (from lyricsgenius) (2.27.1)\n",
      "Collecting beautifulsoup4>=4.6.0\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "     -------------------------------------- 143.0/143.0 kB 8.3 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\melody\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\envs\\melody\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\melody\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\envs\\melody\\lib\\site-packages (from requests>=2.20.0->lyricsgenius) (2.0.12)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, lyricsgenius\n",
      "Successfully installed beautifulsoup4-4.12.2 lyricsgenius-3.0.1 soupsieve-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9zPV_x5B4wdlqwkf6we-rKgebEM9UW82OY0Ijl_1BKqCKGdv0ppBBFOk7VFurjLi\n",
    "# 6TIX-J_XGlNg0UugMtEhFf6CbeKKFBxWcbr2CUuZgk_4B0GC7Uma2BdzEUshfvh2XjJO5LUssSV66a72lzM7PQ\n",
    "\n",
    "# CLIENT ACCESS TOKEN: rcc-qOxLM5fYgEWgzZ_ZroKAEV_ttln5wTe0Rw6waO5WTwQVtZ6PjiiPc8YMugxk\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace 'your_access_token' with the token you got from Genius\n",
    "access_token = 'rcc-qOxLM5fYgEWgzZ_ZroKAEV_ttln5wTe0Rw6waO5WTwQVtZ6PjiiPc8YMugxk'\n",
    "base_url = 'https://api.genius.com'\n",
    "headers = {'Authorization': 'Bearer ' + access_token}\n",
    "\n",
    "def search_song(title):\n",
    "    search_url = base_url + '/search'\n",
    "    data = {'q': title}\n",
    "    response = requests.get(search_url, data=data, headers=headers)\n",
    "\n",
    "    # The response from the API is a JSON string.\n",
    "    json_data = json.loads(response.text)\n",
    "\n",
    "    # The 'hits' field contains the search results.\n",
    "    # Let's print the first result.\n",
    "    first_result = json_data['response']['hits'][0]['result']\n",
    "    print('Title:', first_result['title'])\n",
    "    print('Artist:', first_result['primary_artist']['name'])\n",
    "\n",
    "# Use the function\n",
    "search_song('Bohemian Rhapsody')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace 'your_access_token' with the token you got from Genius\n",
    "access_token = 'rcc-qOxLM5fYgEWgzZ_ZroKAEV_ttln5wTe0Rw6waO5WTwQVtZ6PjiiPc8YMugxk'\n",
    "base_url = 'https://api.genius.com'\n",
    "headers = {'Authorization': 'Bearer ' + access_token}\n",
    "\n",
    "def search_song(title):\n",
    "    search_url = base_url + '/search'\n",
    "    data = {'q': title}\n",
    "    response = requests.get(search_url, data=data, headers=headers)\n",
    "\n",
    "    # Print out the status code and response text\n",
    "    print('Status Code:', response.status_code)\n",
    "    print('Response Text:', response.text)\n",
    "\n",
    "    # The response from the API is a JSON string.\n",
    "    json_data = json.loads(response.text)\n",
    "\n",
    "    # The 'hits' field contains the search results.\n",
    "    # Let's print the first result.\n",
    "    first_result = json_data['response']['hits'][0]['result']\n",
    "    print('Title:', first_result['title'])\n",
    "    print('Artist:', first_result['primary_artist']['name'])\n",
    "\n",
    "# Use the function\n",
    "search_song('Bohemian Rhapsody')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mArtist:\u001b[39m\u001b[39m'\u001b[39m, first_result[\u001b[39m'\u001b[39m\u001b[39mprimary_artist\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[39m# Use the function\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m search_song(\u001b[39m'\u001b[39;49m\u001b[39mBohemian Rhapsody\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m, in \u001b[0;36msearch_song\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     21\u001b[0m json_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext)\n\u001b[0;32m     22\u001b[0m \u001b[39m# The 'hits' field contains the search results.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Let's print the first result.\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m first_result \u001b[39m=\u001b[39m json_data[\u001b[39m'\u001b[39;49m\u001b[39mresponse\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mhits\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTitle:\u001b[39m\u001b[39m'\u001b[39m, first_result[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mArtist:\u001b[39m\u001b[39m'\u001b[39m, first_result[\u001b[39m'\u001b[39m\u001b[39mprimary_artist\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def search_song(title):\n",
    "    # Genius API URL\n",
    "    base_url = \"https://api.genius.com\"\n",
    "    # The headers for the request\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 1aktaQVwQgBmSMeGoFeLcJaAEl08NG9yp3hQrt9R9t8DOpnRRR9YhTPKXIlBAspQ\",  # Replace YOUR_ACCESS_TOKEN with your actual access token\n",
    "        \"User-Agent\": \"CompuServe Classic/1.22\"\n",
    "    }\n",
    "    # The data for the request\n",
    "    data = {\n",
    "        \"q\": title\n",
    "    }\n",
    "    # The search URL\n",
    "    search_url = base_url + \"/search\"\n",
    "    # Make the request\n",
    "    response = requests.get(search_url, data=data, headers=headers)\n",
    "    # The response from the API is a JSON string.\n",
    "    json_data = json.loads(response.text)\n",
    "    # The 'hits' field contains the search results.\n",
    "    # Let's print the first result.\n",
    "    first_result = json_data['response']['hits'][0]['result']\n",
    "    print('Title:', first_result['title'])\n",
    "    print('Artist:', first_result['primary_artist']['name'])\n",
    "\n",
    "# Use the function\n",
    "search_song('Bohemian Rhapsody')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mArtist:\u001b[39m\u001b[39m'\u001b[39m, first_result[\u001b[39m'\u001b[39m\u001b[39mprimary_artist\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[39m# Use the function\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m search_song(\u001b[39m'\u001b[39;49m\u001b[39mBohemian Rhapsody\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[20], line 24\u001b[0m, in \u001b[0;36msearch_song\u001b[1;34m(title)\u001b[0m\n\u001b[0;32m     21\u001b[0m json_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mtext)\n\u001b[0;32m     22\u001b[0m \u001b[39m# The 'hits' field contains the search results.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Let's print the first result.\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m first_result \u001b[39m=\u001b[39m json_data[\u001b[39m'\u001b[39;49m\u001b[39mresponse\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mhits\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTitle:\u001b[39m\u001b[39m'\u001b[39m, first_result[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mArtist:\u001b[39m\u001b[39m'\u001b[39m, first_result[\u001b[39m'\u001b[39m\u001b[39mprimary_artist\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def search_song(title):\n",
    "    # Genius API URL\n",
    "    base_url = \"https://api.genius.com\"\n",
    "    # The headers for the request\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer LAydBIonRi_cMeEayAHevNYA97fksz3zM7RdkswJ9GWL5Iy5wsUN3b121mb9CiFG\",  # Replace YOUR_ACCESS_TOKEN with your actual access token\n",
    "        \"User-Agent\": \"CompuServe Classic/1.22\"\n",
    "    }\n",
    "    # The data for the request\n",
    "    data = {\n",
    "        \"q\": title\n",
    "    }\n",
    "    # The search URL\n",
    "    search_url = base_url + \"/search\"\n",
    "    # Make the request\n",
    "    response = requests.get(search_url, data=data, headers=headers)\n",
    "    # The response from the API is a JSON string.\n",
    "    json_data = json.loads(response.text)\n",
    "    # The 'hits' field contains the search results.\n",
    "    # Let's print the first result.\n",
    "    first_result = json_data['response']['hits'][0]['result']\n",
    "    print('Title:', first_result['title'])\n",
    "    print('Artist:', first_result['primary_artist']['name'])\n",
    "\n",
    "# Use the function\n",
    "search_song('Bohemian Rhapsody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meta': {'status': 200}, 'response': {'hits': [{'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 15, 'api_path': '/songs/2332455', 'artist_names': 'Adele', 'full_title': 'Hello by\\xa0Adele', 'header_image_thumbnail_url': 'https://images.genius.com/6132f071bdd3c86a220e59c6aab609fa.300x300x1.png', 'header_image_url': 'https://images.genius.com/6132f071bdd3c86a220e59c6aab609fa.1000x1000x1.png', 'id': 2332455, 'lyrics_owner_id': 672397, 'lyrics_state': 'complete', 'path': '/Adele-hello-lyrics', 'pyongs_count': 260, 'relationships_index_url': 'https://genius.com/Adele-hello-sample', 'release_date_components': {'year': 2015, 'month': 10, 'day': 23}, 'release_date_for_display': 'October 23, 2015', 'release_date_with_abbreviated_month_for_display': 'Oct. 23, 2015', 'song_art_image_thumbnail_url': 'https://images.genius.com/ed3e7f3e9756fde8d770e78828df544d.300x300x1.png', 'song_art_image_url': 'https://images.genius.com/ed3e7f3e9756fde8d770e78828df544d.1000x1000x1.png', 'stats': {'unreviewed_annotations': 0, 'hot': False, 'pageviews': 5374871}, 'title': 'Hello', 'title_with_featured': 'Hello', 'url': 'https://genius.com/Adele-hello-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/2300', 'header_image_url': 'https://images.genius.com/87aa5d8c32965a10e0e7ad1acb54ba40.1000x287x6.gif', 'id': 2300, 'image_url': 'https://images.genius.com/8a23ab928ccfce13accfde72f1290702.1000x1000x1.jpg', 'is_meme_verified': False, 'is_verified': False, 'name': 'Adele', 'url': 'https://genius.com/artists/Adele'}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 7, 'api_path': '/songs/7260084', 'artist_names': 'Adele', 'full_title': 'Easy On Me by\\xa0Adele', 'header_image_thumbnail_url': 'https://images.genius.com/ef62c92b3e328a3a5b3ff2383eca6b6c.300x300x1.png', 'header_image_url': 'https://images.genius.com/ef62c92b3e328a3a5b3ff2383eca6b6c.1000x1000x1.png', 'id': 7260084, 'lyrics_owner_id': 3489629, 'lyrics_state': 'complete', 'path': '/Adele-easy-on-me-lyrics', 'pyongs_count': 156, 'relationships_index_url': 'https://genius.com/Adele-easy-on-me-sample', 'release_date_components': {'year': 2021, 'month': 10, 'day': 15}, 'release_date_for_display': 'October 15, 2021', 'release_date_with_abbreviated_month_for_display': 'Oct. 15, 2021', 'song_art_image_thumbnail_url': 'https://images.genius.com/ef62c92b3e328a3a5b3ff2383eca6b6c.300x300x1.png', 'song_art_image_url': 'https://images.genius.com/ef62c92b3e328a3a5b3ff2383eca6b6c.1000x1000x1.png', 'stats': {'unreviewed_annotations': 0, 'concurrents': 2, 'hot': False, 'pageviews': 2725246}, 'title': 'Easy On Me', 'title_with_featured': 'Easy On Me', 'url': 'https://genius.com/Adele-easy-on-me-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/2300', 'header_image_url': 'https://images.genius.com/87aa5d8c32965a10e0e7ad1acb54ba40.1000x287x6.gif', 'id': 2300, 'image_url': 'https://images.genius.com/8a23ab928ccfce13accfde72f1290702.1000x1000x1.jpg', 'is_meme_verified': False, 'is_verified': False, 'name': 'Adele', 'url': 'https://genius.com/artists/Adele'}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 6, 'api_path': '/songs/51294', 'artist_names': 'Adele', 'full_title': 'Someone Like You by\\xa0Adele', 'header_image_thumbnail_url': 'https://images.genius.com/b72029e91a73dbe50e6a063e83f0801c.300x300x1.jpg', 'header_image_url': 'https://images.genius.com/b72029e91a73dbe50e6a063e83f0801c.1000x1000x1.jpg', 'id': 51294, 'lyrics_owner_id': 19432, 'lyrics_state': 'complete', 'path': '/Adele-someone-like-you-lyrics', 'pyongs_count': 221, 'relationships_index_url': 'https://genius.com/Adele-someone-like-you-sample', 'release_date_components': {'year': 2011, 'month': 1, 'day': 24}, 'release_date_for_display': 'January 24, 2011', 'release_date_with_abbreviated_month_for_display': 'Jan. 24, 2011', 'song_art_image_thumbnail_url': 'https://images.genius.com/85af0d427b22e876cd10a7ca128399c8.300x300x1.jpg', 'song_art_image_url': 'https://images.genius.com/85af0d427b22e876cd10a7ca128399c8.1000x1000x1.jpg', 'stats': {'unreviewed_annotations': 0, 'concurrents': 2, 'hot': False, 'pageviews': 1943765}, 'title': 'Someone Like You', 'title_with_featured': 'Someone Like You', 'url': 'https://genius.com/Adele-someone-like-you-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/2300', 'header_image_url': 'https://images.genius.com/87aa5d8c32965a10e0e7ad1acb54ba40.1000x287x6.gif', 'id': 2300, 'image_url': 'https://images.genius.com/8a23ab928ccfce13accfde72f1290702.1000x1000x1.jpg', 'is_meme_verified': False, 'is_verified': False, 'name': 'Adele', 'url': 'https://genius.com/artists/Adele'}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 19, 'api_path': '/songs/61289', 'artist_names': 'Adele', 'full_title': 'Rolling in the Deep by\\xa0Adele', 'header_image_thumbnail_url': 'https://images.genius.com/b72029e91a73dbe50e6a063e83f0801c.300x300x1.jpg', 'header_image_url': 'https://images.genius.com/b72029e91a73dbe50e6a063e83f0801c.1000x1000x1.jpg', 'id': 61289, 'lyrics_owner_id': 56449, 'lyrics_state': 'complete', 'path': '/Adele-rolling-in-the-deep-lyrics', 'pyongs_count': 153, 'relationships_index_url': 'https://genius.com/Adele-rolling-in-the-deep-sample', 'release_date_components': {'year': 2010, 'month': 11, 'day': 29}, 'release_date_for_display': 'November 29, 2010', 'release_date_with_abbreviated_month_for_display': 'Nov. 29, 2010', 'song_art_image_thumbnail_url': 'https://images.genius.com/68266747aa7bd27b0055506437c13438.300x300x1.jpg', 'song_art_image_url': 'https://images.genius.com/68266747aa7bd27b0055506437c13438.1000x1000x1.jpg', 'stats': {'unreviewed_annotations': 3, 'concurrents': 3, 'hot': False, 'pageviews': 1334876}, 'title': 'Rolling in the Deep', 'title_with_featured': 'Rolling in the Deep', 'url': 'https://genius.com/Adele-rolling-in-the-deep-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/2300', 'header_image_url': 'https://images.genius.com/87aa5d8c32965a10e0e7ad1acb54ba40.1000x287x6.gif', 'id': 2300, 'image_url': 'https://images.genius.com/8a23ab928ccfce13accfde72f1290702.1000x1000x1.jpg', 'is_meme_verified': False, 'is_verified': False, 'name': 'Adele', 'url': 'https://genius.com/artists/Adele'}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 10, 'api_path': '/songs/2336628', 'artist_names': 'Adele', 'full_title': 'When We Were Young by\\xa0Adele', 'header_image_thumbnail_url': 'https://images.genius.com/701035c95312a64d18373b3ef4d4f556.300x300x1.png', 'header_image_url': 'https://images.genius.com/701035c95312a64d18373b3ef4d4f556.1000x1000x1.png', 'id': 2336628, 'lyrics_owner_id': 104344, 'lyrics_state': 'complete', 'path': '/Adele-when-we-were-young-lyrics', 'pyongs_count': 93, 'relationships_index_url': 'https://genius.com/Adele-when-we-were-young-sample', 'release_date_components': {'year': 2015, 'month': 11, 'day': 20}, 'release_date_for_display': 'November 20, 2015', 'release_date_with_abbreviated_month_for_display': 'Nov. 20, 2015', 'song_art_image_thumbnail_url': 'https://images.genius.com/ed4a2753ac88891a7ca5d514e9bd7c23.300x300x1.png', 'song_art_image_url': 'https://images.genius.com/ed4a2753ac88891a7ca5d514e9bd7c23.1000x1000x1.png', 'stats': {'unreviewed_annotations': 0, 'hot': False, 'pageviews': 1208586}, 'title': 'When We Were Young', 'title_with_featured': 'When We Were Young', 'url': 'https://genius.com/Adele-when-we-were-young-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/2300', 'header_image_url': 'https://images.genius.com/87aa5d8c32965a10e0e7ad1acb54ba40.1000x287x6.gif', 'id': 2300, 'image_url': 'https://images.genius.com/8a23ab928ccfce13accfde72f1290702.1000x1000x1.jpg', 'is_meme_verified': False, 'is_verified': False, 'name': 'Adele', 'url': 'https://genius.com/artists/Adele'}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 10, 'api_path': '/songs/62493', 'artist_names': 'Adele', 'full_title': 'Set Fire to the Rain by\\xa0Adele', 'header_image_thumbnail_url': 'https://images.genius.com/b72029e91a73dbe50e6a063e83f0801c.300x300x1.jpg', 'header_image_url': 'https://images.genius.com/b72029e91a73dbe50e6a063e83f0801c.1000x1000x1.jpg', 'id': 62493, 'lyrics_owner_id': 53179, 'lyrics_state': 'complete', 'path': '/Adele-set-fire-to-the-rain-lyrics', 'pyongs_count': 106, 'relationships_index_url': 'https://genius.com/Adele-set-fire-to-the-rain-sample', 'release_date_components': {'year': 2011, 'month': 1, 'day': 24}, 'release_date_for_display': 'January 24, 2011', 'release_date_with_abbreviated_month_for_display': 'Jan. 24, 2011', 'song_art_image_thumbnail_url': 'https://images.genius.com/119a8e476b06fabac4c39a3d7acf59c1.300x300x1.jpg', 'song_art_image_url': 'https://images.genius.com/119a8e476b06fabac4c39a3d7acf59c1.1000x1000x1.jpg', 'stats': {'unreviewed_annotations': 2, 'concurrents': 2, 'hot': False, 'pageviews': 1175119}, 'title': 'Set Fire to the Rain', 'title_with_featured': 'Set Fire to the Rain', 'url': 'https://genius.com/Adele-set-fire-to-the-rain-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/2300', 'header_image_url': 'https://images.genius.com/87aa5d8c32965a10e0e7ad1acb54ba40.1000x287x6.gif', 'id': 2300, 'image_url': 'https://images.genius.com/8a23ab928ccfce13accfde72f1290702.1000x1000x1.jpg', 'is_meme_verified': False, 'is_verified': False, 'name': 'Adele', 'url': 'https://genius.com/artists/Adele'}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 11, 'api_path': '/songs/2336647', 'artist_names': 'Adele', 'full_title': 'All I Ask by\\xa0Adele', 'header_image_thumbnail_url': 'https://images.genius.com/b224971f00520e26ca7c6d4ab92adab5.300x300x1.png', 'header_image_url': 'https://images.genius.com/b224971f00520e26ca7c6d4ab92adab5.1000x1000x1.png', 'id': 2336647, 'lyrics_owner_id': 104344, 'lyrics_state': 'complete', 'path': '/Adele-all-i-ask-lyrics', 'pyongs_count': 44, 'relationships_index_url': 'https://genius.com/Adele-all-i-ask-sample', 'release_date_components': {'year': 2015, 'month': 11, 'day': 20}, 'release_date_for_display': 'November 20, 2015', 'release_date_with_abbreviated_month_for_display': 'Nov. 20, 2015', 'song_art_image_thumbnail_url': 'https://images.genius.com/b224971f00520e26ca7c6d4ab92adab5.300x300x1.png', 'song_art_image_url': 'https://images.genius.com/b224971f00520e26ca7c6d4ab92adab5.1000x1000x1.png', 'stats': {'unreviewed_annotations': 1, 'hot': False, 'pageviews': 1094212}, 'title': 'All I Ask', 'title_with_featured': 'All I Ask', 'url': 'https://genius.com/Adele-all-i-ask-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/2300', 'header_image_url': 'https://images.genius.com/87aa5d8c32965a10e0e7ad1acb54ba40.1000x287x6.gif', 'id': 2300, 'image_url': 'https://images.genius.com/8a23ab928ccfce13accfde72f1290702.1000x1000x1.jpg', 'is_meme_verified': False, 'is_verified': False, 'name': 'Adele', 'url': 'https://genius.com/artists/Adele'}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 16, 'api_path': '/songs/381520', 'artist_names': 'Melanie Martinez', 'full_title': 'Dollhouse by\\xa0Melanie\\xa0Martinez', 'header_image_thumbnail_url': 'https://images.genius.com/3f1f312b35d0299c08d35325921f5719.300x300x1.jpg', 'header_image_url': 'https://images.genius.com/3f1f312b35d0299c08d35325921f5719.1000x1000x1.jpg', 'id': 381520, 'lyrics_owner_id': 676185, 'lyrics_state': 'complete', 'path': '/Melanie-martinez-dollhouse-lyrics', 'pyongs_count': 137, 'relationships_index_url': 'https://genius.com/Melanie-martinez-dollhouse-sample', 'release_date_components': {'year': 2014, 'month': 2, 'day': 9}, 'release_date_for_display': 'February 9, 2014', 'release_date_with_abbreviated_month_for_display': 'Feb. 9, 2014', 'song_art_image_thumbnail_url': 'https://images.genius.com/ac5db42913f6da2749c7120e089a387a.300x300x1.png', 'song_art_image_url': 'https://images.genius.com/ac5db42913f6da2749c7120e089a387a.1000x1000x1.png', 'stats': {'unreviewed_annotations': 5, 'hot': False, 'pageviews': 826447}, 'title': 'Dollhouse', 'title_with_featured': 'Dollhouse', 'url': 'https://genius.com/Melanie-martinez-dollhouse-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/153995', 'header_image_url': 'https://images.genius.com/c6234bbc4179524115a6cac915e4fe42.1000x429x1.jpg', 'id': 153995, 'image_url': 'https://images.genius.com/0d4d11bc2350ba993b6d92be4c94e042.557x557x1.jpg', 'is_meme_verified': False, 'is_verified': True, 'name': 'Melanie Martinez', 'url': 'https://genius.com/artists/Melanie-martinez', 'iq': 245}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 26, 'api_path': '/songs/352918', 'artist_names': 'Idina Menzel', 'full_title': 'Let It Go by\\xa0Idina\\xa0Menzel', 'header_image_thumbnail_url': 'https://images.genius.com/a6010ba5a7acf9357c5bc567b051b0f6.300x300x1.jpg', 'header_image_url': 'https://images.genius.com/a6010ba5a7acf9357c5bc567b051b0f6.800x800x1.jpg', 'id': 352918, 'lyrics_owner_id': 478016, 'lyrics_state': 'complete', 'path': '/Idina-menzel-let-it-go-lyrics', 'pyongs_count': 259, 'relationships_index_url': 'https://genius.com/Idina-menzel-let-it-go-sample', 'release_date_components': {'year': 2013, 'month': 11, 'day': 27}, 'release_date_for_display': 'November 27, 2013', 'release_date_with_abbreviated_month_for_display': 'Nov. 27, 2013', 'song_art_image_thumbnail_url': 'https://images.genius.com/a6010ba5a7acf9357c5bc567b051b0f6.300x300x1.jpg', 'song_art_image_url': 'https://images.genius.com/a6010ba5a7acf9357c5bc567b051b0f6.800x800x1.jpg', 'stats': {'unreviewed_annotations': 0, 'concurrents': 2, 'hot': False, 'pageviews': 830236}, 'title': 'Let It Go', 'title_with_featured': 'Let It Go', 'url': 'https://genius.com/Idina-menzel-let-it-go-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/33242', 'header_image_url': 'https://images.genius.com/8dae7cbdcc7da145e8a154a4f5f964c3.700x500x1.jpg', 'id': 33242, 'image_url': 'https://images.genius.com/b2782069e15cfac2ac6c7a25cd4632cd.500x500x1.jpg', 'is_meme_verified': False, 'is_verified': True, 'name': 'Idina Menzel', 'url': 'https://genius.com/artists/Idina-menzel', 'iq': 667}}}, {'highlights': [], 'index': 'song', 'type': 'song', 'result': {'annotation_count': 8, 'api_path': '/songs/2336620', 'artist_names': 'Adele', 'full_title': 'Send My Love (To Your New Lover) by\\xa0Adele', 'header_image_thumbnail_url': 'https://images.genius.com/6132f071bdd3c86a220e59c6aab609fa.300x300x1.png', 'header_image_url': 'https://images.genius.com/6132f071bdd3c86a220e59c6aab609fa.1000x1000x1.png', 'id': 2336620, 'lyrics_owner_id': 266331, 'lyrics_state': 'complete', 'path': '/Adele-send-my-love-to-your-new-lover-lyrics', 'pyongs_count': 60, 'relationships_index_url': 'https://genius.com/Adele-send-my-love-to-your-new-lover-sample', 'release_date_components': {'year': 2015, 'month': 11, 'day': 20}, 'release_date_for_display': 'November 20, 2015', 'release_date_with_abbreviated_month_for_display': 'Nov. 20, 2015', 'song_art_image_thumbnail_url': 'https://images.genius.com/e20c352a7871b6dcadf70428dadfd46b.300x300x1.jpg', 'song_art_image_url': 'https://images.genius.com/e20c352a7871b6dcadf70428dadfd46b.1000x1000x1.jpg', 'stats': {'unreviewed_annotations': 0, 'hot': False, 'pageviews': 715557}, 'title': 'Send My Love (To Your New Lover)', 'title_with_featured': 'Send My Love (To Your New Lover)', 'url': 'https://genius.com/Adele-send-my-love-to-your-new-lover-lyrics', 'featured_artists': [], 'primary_artist': {'api_path': '/artists/2300', 'header_image_url': 'https://images.genius.com/87aa5d8c32965a10e0e7ad1acb54ba40.1000x287x6.gif', 'id': 2300, 'image_url': 'https://images.genius.com/8a23ab928ccfce13accfde72f1290702.1000x1000x1.jpg', 'is_meme_verified': False, 'is_verified': False, 'name': 'Adele', 'url': 'https://genius.com/artists/Adele'}}}]}}\n",
      "Title: Hello\n",
      "Artist: Adele\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def search_song(title):\n",
    "    # Genius API URL\n",
    "    base_url = \"https://api.genius.com\"\n",
    "    # The headers for the request\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 1aktaQVwQgBmSMeGoFeLcJaAEl08NG9yp3hQrt9R9t8DOpnRRR9YhTPKXIlBAspQ\",  # Replace YOUR_ACCESS_TOKEN with your actual access token\n",
    "        \"User-Agent\": \"CompuServe Classic/1.22\"\n",
    "    }\n",
    "    # The data for the request\n",
    "    data = {\n",
    "        \"q\": title\n",
    "    }\n",
    "    # The search URL\n",
    "    search_url = base_url + \"/search\"\n",
    "    # Make the request\n",
    "    response = requests.get(search_url, data=data, headers=headers)\n",
    "    # The response from the API is a JSON string.\n",
    "    json_data = json.loads(response.text)\n",
    "    # The 'hits' field contains the search results.\n",
    "    # Let's print the first result.\n",
    "    print(json_data)  # Print the entire response\n",
    "    if json_data['response']['hits']:\n",
    "        first_result = json_data['response']['hits'][0]['result']\n",
    "        print('Title:', first_result['title'])\n",
    "        print('Artist:', first_result['primary_artist']['name'])\n",
    "        # Save the JSON data to a file\n",
    "        with open('song_data.json', 'w') as f:\n",
    "            json.dump(json_data, f)\n",
    "    else:\n",
    "        print('No results found for', title)\n",
    "\n",
    "# Use the function\n",
    "search_song('Adele')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL for lyrics: https://genius.com/Adele-someone-like-you-lyrics\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('song_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Search for the song\n",
    "for hit in data['response']['hits']:\n",
    "    if hit['result']['title'].lower() == 'someone like you':\n",
    "        song_url = hit['result']['url']\n",
    "        print('URL for lyrics:', song_url)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(r_html, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Find the lyrics on the page and print them\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m lyrics \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfind(\u001b[39m\"\u001b[39;49m\u001b[39mdiv\u001b[39;49m\u001b[39m\"\u001b[39;49m, class_\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlyrics\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mget_text()\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(lyrics)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Make a request to the website\n",
    "r = requests.get(song_url)\n",
    "r_html = r.text\n",
    "\n",
    "# Create a BeautifulSoup object and specify the parser\n",
    "soup = BeautifulSoup(r_html, \"html.parser\")\n",
    "\n",
    "# Find the lyrics on the page and print them\n",
    "lyrics = soup.find(\"div\", class_=\"lyrics\").get_text()\n",
    "print(lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics not found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://genius.com/Adele-someone-like-you-lyrics\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# The lyrics are within a div with the class \"lyrics\"\n",
    "lyrics_div = soup.find('div', class_='lyrics')\n",
    "if lyrics_div:\n",
    "    lyrics = lyrics_div.get_text(strip=True)\n",
    "    print(lyrics)\n",
    "else:\n",
    "    print(\"Lyrics not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"In The End\" by Linkin Park...\n",
      "Done.\n",
      "210 ContributorsTranslationsTürkçeEspañolPortuguêsItalianoDeutschIn the End Lyrics[Verse 1: Mike Shinoda, Chester Bennington, Both]\n",
      "It starts with one\n",
      "One thing, I don't know why\n",
      "It doesn't even matter how hard you try\n",
      "Keep that in mind, I designed this rhyme\n",
      "To explain in due time, all I know\n",
      "Time is a valuable thing\n",
      "Watch it fly by as the pendulum swings\n",
      "Watch it count down to the end of the day\n",
      "The clock ticks life away, it's so unreal\n",
      "Didn't look out below\n",
      "Watch the time go right out the window\n",
      "Tryin' to hold on, d-didn't even know\n",
      "I wasted it all just to watch you go\n",
      "\n",
      "[Pre-Chorus: Mike Shinoda]\n",
      "I kept everything inside\n",
      "And even though I tried, it all fell apart\n",
      "What it meant to me will eventually be\n",
      "A memory of a time when I tried so hard\n",
      "\n",
      "[Chorus: Chester Bennington]\n",
      "I tried so hard and got so far\n",
      "But in the end, it doesn't even matter\n",
      "I had to fall to lose it all\n",
      "But in the end, it doesn't even matter\n",
      "You might also like[Verse 2: Mike Shinoda, Both]\n",
      "One thing, I don't know why\n",
      "It doesn't even matter how hard you try\n",
      "Keep that in mind, I designed this rhyme\n",
      "To remind myself how I tried so hard\n",
      "In spite of the way you were mockin' me\n",
      "Actin' like I was part of your property\n",
      "Remembering all the times you fought with me\n",
      "I'm surprised it got so far\n",
      "Things aren't the way they were before\n",
      "You wouldn't even recognize me anymore\n",
      "Not that you knew me back then\n",
      "But it all comes back to me in the end\n",
      "\n",
      "[Pre-Chorus: Mike Shinoda]\n",
      "You kept everything inside\n",
      "And even though I tried, it all fell apart\n",
      "What it meant to me will eventually be\n",
      "A memory of a time when I tried so hard\n",
      "\n",
      "[Chorus: Chester Bennington]\n",
      "I tried so hard and got so far\n",
      "But in the end, it doesn't even matter\n",
      "I had to fall to lose it all\n",
      "But in the end, it doesn't even matter\n",
      "[Bridge: Chester Bennington]\n",
      "I've put my trust in you\n",
      "Pushed as far as I can go\n",
      "For all this, there's only one thing you should know\n",
      "I've put my trust in you\n",
      "Pushed as far as I can go\n",
      "For all this, there's only one thing you should know\n",
      "\n",
      "[Chorus: Chester Bennington]\n",
      "I tried so hard and got so far\n",
      "But in the end, it doesn't even matter\n",
      "I had to fall to lose it all\n",
      "But in the end, it doesn't even matter286Embed\n"
     ]
    }
   ],
   "source": [
    "import lyricsgenius as genius\n",
    "\n",
    "# Create a Genius object\n",
    "api = genius.Genius('LAydBIonRi_cMeEayAHevNYA97fksz3zM7RdkswJ9GWL5Iy5wsUN3b121mb9CiFG')\n",
    "\n",
    "# Search for a song\n",
    "song = api.search_song('In The End', 'Linkin Park')\n",
    "\n",
    "# Print the lyrics\n",
    "print(song.lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Alexander All Alone\" by Andy Shauf...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lyricsgenius\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "\n",
    "token = \"LAydBIonRi_cMeEayAHevNYA97fksz3zM7RdkswJ9GWL5Iy5wsUN3b121mb9CiFG\"\n",
    "genius = lyricsgenius.Genius(token)\n",
    "\n",
    "def getSongURL(genius, title, artist) -> str:\n",
    "    song = genius.search_song(title, artist)\n",
    "    return \"https://genius.com/\" + song.path\n",
    "\n",
    "def removePattern(s, patterns):\n",
    "    for pattern in patterns:\n",
    "        r = re.compile(pattern)\n",
    "        s = r.sub(\"\", s)\n",
    "    return s\n",
    "\n",
    "def getLyricsFromPage(url):\n",
    "    page = requests.get(url)\n",
    "    attrs = {\"data-lyrics-container\": \"true\"}\n",
    "    html = bs4.BeautifulSoup(page.text, \"html.parser\").find(attrs=attrs)\n",
    "\n",
    "    html = removePattern(str(html), [r\"<(span|a|div)(.|\\n)*?>\", r\"</(span|a|div)>\"])\n",
    "\n",
    "    tags = re.findall(r\"[.+?]\", html)\n",
    "    text = re.split(r\"[.+?]\", html)\n",
    "    text = [t for t in text if t != \"\"]\n",
    "\n",
    "    lyrics = {}\n",
    "\n",
    "    for i in range(len(tags)):\n",
    "        block = re.split(r\"<br/>\", text[i])\n",
    "        block = [b for b in block if b != \"\"]\n",
    "        lyrics[tags[i]] = block\n",
    "\n",
    "    return lyrics\n",
    "\n",
    "url = getSongURL(genius, \"Alexander All Alone\", \"Andy Shauf\")\n",
    "lyrics = getLyricsFromPage(url)\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \"Alexander All Alone\" by Andy Shauf...\n",
      "Done.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import lyricsgenius\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "\n",
    "token = \"LAydBIonRi_cMeEayAHevNYA97fksz3zM7RdkswJ9GWL5Iy5wsUN3b121mb9CiFG\"\n",
    "genius = lyricsgenius.Genius(token)\n",
    "\n",
    "def getSongURL(genius, title, artist) -> str:\n",
    "    song = genius.search_song(title, artist)\n",
    "    return \"https://genius.com/\" + song.path\n",
    "\n",
    "def removePattern(s, patterns):\n",
    "    for pattern in patterns:\n",
    "        r = re.compile(pattern)\n",
    "        s = r.sub(\"\", s)\n",
    "    return s\n",
    "\n",
    "def getLyricsFromPage(url):\n",
    "    page = requests.get(url)\n",
    "    attrs = {\"data-lyrics-container\": \"true\"}\n",
    "    html = bs4.BeautifulSoup(page.text, \"html.parser\").find(attrs=attrs)\n",
    "\n",
    "    html = removePattern(str(html), [r\"<(span|a|div)(.|\\n)*?>\", r\"</(span|a|div)>\"])\n",
    "\n",
    "    tags = re.findall(r\"[.+?]\", html)\n",
    "    text = re.split(r\"[.+?]\", html)\n",
    "    text = [t for t in text if t != \"\"]\n",
    "\n",
    "    lyrics = []\n",
    "\n",
    "    for i in range(len(tags)):\n",
    "        block = re.split(r\"<br/>\", text[i])\n",
    "        block = [b for b in block if b != \"\"]\n",
    "        lyrics.append({\n",
    "            tags[i]: block\n",
    "        })\n",
    "\n",
    "    return lyrics\n",
    "\n",
    "url = getSongURL(genius, \"Alexander All Alone\", \"Andy Shauf\")\n",
    "lyrics = getLyricsFromPage(url)\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to C:\\Users\\L5\n",
      "[nltk_data]     15IMH6-58IX\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('semcor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor\n",
    "\n",
    "# semcor.sents() gives you the sentences in the corpus, where each sentence is a list of words\n",
    "sentences = semcor.sents()\n",
    "\n",
    "# semcor.tagged_sents(tag='sem') gives you the sentences with semantic (sense) tags\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "senses_sentences = [[get_sense(tag) for tag in sent] for sent in tagged_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Suppose you have the following contexts for the word 'bank'\n",
    "contexts = [\n",
    "    \"I deposited my money in the bank\",\n",
    "    \"I sat by the bank of the river\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)  # binary=True makes the representation a bag-of-words\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "\n",
    "# X is a sparse matrix where each row is a bag-of-words representation of a context\n",
    "# You can convert it to an array with X.toarray(), or access individual elements with X[i, j]\n",
    "\n",
    "# The words corresponding to each column in X can be obtained with vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_lower = [[word.lower() for word in sentence] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert list of tokens to strings\n",
    "sentences_str = [\" \".join(sentence) for sentence in sentences_lower]\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)  # binary=True makes the representation a bag-of-words\n",
    "X = vectorizer.fit_transform(sentences_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "sense_sentences = [[get_sense(tag) for tag in sent] for sent in tagged_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [sense\n",
    "     for sense_sentence in sense_sentences\n",
    "     for sense in sense_sentence\n",
    "     if sense is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [37176, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2556\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \n\u001b[0;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [37176, 0]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# Create a bag-of-words representation of the contexts\u001b[39;00m\n\u001b[0;32m     34\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 35\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(contexts)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Convert senses to a format suitable for scikit-learn\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m# (Here we simply convert each Synset to its corresponding string)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m y \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(sense) \u001b[39mfor\u001b[39;00m sense \u001b[39min\u001b[39;00m senses]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "# Function to get sense from a tag\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get tagged sentences from SemCor\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "# Create lists for contexts and senses\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "# For each tagged sentence...\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    # For each tag in the sentence...\n",
    "    for tag in tagged_sentence:\n",
    "        # If the tag is a Lemma (has a sense)...\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            # Add the sense to the senses list\n",
    "            senses.append(sense)\n",
    "            \n",
    "            # Add the sentence (as a string) to the contexts list\n",
    "            sentence = \" \".join(word for word, tag in tagged_sentence if isinstance(tag, str))\n",
    "            contexts.append(sentence)\n",
    "\n",
    "# Create a bag-of-words representation of the contexts\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "\n",
    "# Convert senses to a format suitable for scikit-learn\n",
    "# (Here we simply convert each Synset to its corresponding string)\n",
    "y = [str(sense) for sense in senses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 17\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(word \u001b[39mfor\u001b[39;49;00m word, tag \u001b[39min\u001b[39;49;00m tagged_sentence \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(tag, \u001b[39mstr\u001b[39;49m) \u001b[39mor\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(tag, Lemma))\n\u001b[0;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     20\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     14\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 17\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(word \u001b[39mfor\u001b[39;00m word, tag \u001b[39min\u001b[39;00m tagged_sentence \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tag, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(tag, Lemma))\n\u001b[0;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     20\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(word for word, tag in tagged_sentence if isinstance(tag, str) or isinstance(tag, Lemma))\n",
    "    \n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "\n",
    "y = [str(sense) for sense in senses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m             contexts\u001b[39m.\u001b[39mappend(sentence)\n\u001b[0;32m     25\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(contexts)\n\u001b[0;32m     28\u001b[0m y \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(sense) \u001b[39mfor\u001b[39;00m sense \u001b[39min\u001b[39;00m senses]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(\" \".join(tag.leaves()) for tag in tagged_sentence if isinstance(tag, str) or isinstance(tag, Lemma))\n",
    "    \n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "\n",
    "y = [str(sense) for sense in senses]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SemcorSentence' object has no attribute 'leaves'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 17\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tagged_sentence\u001b[39m.\u001b[39;49mleaves())\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     19\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SemcorSentence' object has no attribute 'leaves'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(word for word in tagged_sentence.leaves())\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m(contexts[:\u001b[39m10\u001b[39m])\n\u001b[0;32m     25\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(contexts)\n\u001b[0;32m     27\u001b[0m y \u001b[39m=\u001b[39m senses\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(tag.leaves()[0] for tag in tagged_sentence if hasattr(tag, 'leaves'))\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 17\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mname() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(tag, Lemma) \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mleaves()) \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(tag, \u001b[39m'\u001b[39;49m\u001b[39mleaves\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39melse\u001b[39;49;00m tag \u001b[39mfor\u001b[39;49;00m tag \u001b[39min\u001b[39;49;00m tagged_sentence)\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     19\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(tag.name() if isinstance(tag, Lemma) else \" \".join(tag.leaves()) if hasattr(tag, 'leaves') else tag for tag in tagged_sentence)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 17\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mname() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(tag, Lemma) \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(word \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m tag\u001b[39m.\u001b[39;49mleaves()) \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(tag, \u001b[39m'\u001b[39;49m\u001b[39mleaves\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39melse\u001b[39;49;00m tag \u001b[39mfor\u001b[39;49;00m tag \u001b[39min\u001b[39;49;00m tagged_sentence)\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     19\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(tag.name() if isinstance(tag, Lemma) else \" \".join(word for word in tag.leaves()) if hasattr(tag, 'leaves') else tag for tag in tagged_sentence)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 17\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mname() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(tag, Lemma) \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mleaves()[\u001b[39m0\u001b[39;49m]) \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(tag, \u001b[39m'\u001b[39;49m\u001b[39mleaves\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39melse\u001b[39;49;00m tag \u001b[39mfor\u001b[39;49;00m tag \u001b[39min\u001b[39;49;00m tagged_sentence)\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     19\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(tag.name() if isinstance(tag, Lemma) else \" \".join(tag.leaves()[0]) if hasattr(tag, 'leaves') else tag for tag in tagged_sentence)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 17\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mname() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(tag, Lemma) \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mleaves()) \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(tag, \u001b[39m'\u001b[39;49m\u001b[39mleaves\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39melse\u001b[39;49;00m tag \u001b[39mfor\u001b[39;49;00m tag \u001b[39min\u001b[39;49;00m tagged_sentence)\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     19\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(tag.name() if isinstance(tag, Lemma) else \" \".join(tag.leaves()) if hasattr(tag, 'leaves') else tag for tag in tagged_sentence)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 20\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mname() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(tag, Lemma) \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(flatten(tag\u001b[39m.\u001b[39;49mleaves())) \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(tag, \u001b[39m'\u001b[39;49m\u001b[39mleaves\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39melse\u001b[39;49;00m tag \u001b[39mfor\u001b[39;49;00m tag \u001b[39min\u001b[39;49;00m tagged_sentence)\n\u001b[0;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     22\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def flatten(lst):\n",
    "    return [item for sublist in lst for item in sublist]\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(tag.name() if isinstance(tag, Lemma) else \" \".join(flatten(tag.leaves())) if hasattr(tag, 'leaves') else tag for tag in tagged_sentence)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m senses \u001b[39m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m tagged_sentence \u001b[39min\u001b[39;00m tagged_sentences:\n\u001b[1;32m---> 25\u001b[0m     sentence \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(tag\u001b[39m.\u001b[39;49mname() \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(tag, Lemma) \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(flatten(tag\u001b[39m.\u001b[39;49mleaves())) \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(tag, \u001b[39m'\u001b[39;49m\u001b[39mleaves\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39melse\u001b[39;49;00m tag \u001b[39mfor\u001b[39;49;00m tag \u001b[39min\u001b[39;49;00m tagged_sentence)\n\u001b[0;32m     26\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m tagged_sentence:\n\u001b[0;32m     27\u001b[0m         sense \u001b[39m=\u001b[39m get_sense(tag)\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def flatten(lis):\n",
    "    for item in lis:\n",
    "        if isinstance(item, list) and not isinstance(item, str):\n",
    "            for x in flatten(item):\n",
    "                yield x\n",
    "        else:        \n",
    "            yield item\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence = \" \".join(tag.name() if isinstance(tag, Lemma) else \" \".join(flatten(tag.leaves())) if hasattr(tag, 'leaves') else tag for tag in tagged_sentence)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', 'Atlanta', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']\n",
      "Senses: []\n",
      "Words: ['jury', 'further', 'said', 'term', 'end', 'presentments', 'City', 'Executive', 'Committee', 'had', 'over-all', 'charge', 'election', 'deserves', 'praise', 'thanks', 'City', 'of', 'Atlanta', 'manner', 'election', 'conducted']\n",
      "Senses: []\n",
      "Words: ['September', 'October', 'term', 'jury', 'charged', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'investigate', 'reports', 'possible', 'irregularities', 'hard-fought', 'primary', 'won', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.']\n",
      "Senses: []\n",
      "Words: ['Only', 'relative', 'handful', 'such', 'reports', 'received', 'jury', 'said', 'considering', 'widespread', 'interest', 'election', 'number', 'voters', 'size', 'city']\n",
      "Senses: []\n",
      "Words: ['jury', 'said', 'find', 'Georgia', 'registration', 'election', 'laws', 'are', 'outmoded', 'inadequate', 'often', 'ambiguous']\n",
      "Senses: []\n",
      "Words: ['recommended', 'Fulton', 'legislators', 'act', 'have', 'laws', 'studied', 'revised', 'end', 'modernizing', 'improving']\n",
      "Senses: []\n",
      "Words: ['grand', 'jury', 'commented', 'number', 'other', 'topics', 'Atlanta', 'Fulton', 'County', 'purchasing', 'departments', 'said', 'are', 'well', 'operated', 'follow', 'generally', 'accepted', 'practices', 'inure', 'best', 'interest', 'governments']\n",
      "Senses: []\n",
      "Words: ['However', 'jury', 'said', 'believes', 'two', 'offices', 'combined', 'achieve', 'greater', 'efficiency', 'reduce', 'cost', 'administration']\n",
      "Senses: []\n",
      "Words: ['City', 'Purchasing', 'Department', 'jury', 'said', 'is', 'lacking', 'experienced', 'clerical', 'personnel', 'result', 'city', 'personnel', 'policies']\n",
      "Senses: []\n",
      "Words: ['urged', 'city', 'take', 'steps', 'remedy', 'problem']\n",
      "Senses: []\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_words(tag):\n",
    "    if isinstance(tag, str):\n",
    "        return [tag]\n",
    "    elif isinstance(tag, Tree):\n",
    "        return [word for subtree in tag for word in extract_words(subtree)]\n",
    "    elif isinstance(tag, Lemma):\n",
    "        return [tag.name()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')[:10]\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    words = []\n",
    "    senses = []\n",
    "    for tag in tagged_sentence:\n",
    "        words.extend(extract_words(tag))\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "    print(f\"Words: {words}\")\n",
    "    print(f\"Senses: {senses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mprint\u001b[39m(contexts[:\u001b[39m10\u001b[39m])\n\u001b[0;32m     38\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(contexts)\n\u001b[0;32m     40\u001b[0m y \u001b[39m=\u001b[39m senses\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_words(tag):\n",
    "    if isinstance(tag, str):\n",
    "        return [tag]\n",
    "    elif isinstance(tag, Tree):\n",
    "        return [word for subtree in tag for word in extract_words(subtree)]\n",
    "    elif isinstance(tag, Lemma):\n",
    "        return [tag.name()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    words = [word for tag in tagged_sentence for word in extract_words(tag)]\n",
    "    sentence = \" \".join(words)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lemma tags: 0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.corpus import semcor\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Lemma):\n",
    "        return tag.synset().name()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_words(tag):\n",
    "    if isinstance(tag, str):\n",
    "        return [tag]\n",
    "    elif isinstance(tag, Tree):\n",
    "        return [word for subtree in tag for word in extract_words(subtree)]\n",
    "    elif isinstance(tag, Lemma):\n",
    "        return [tag.name()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "lemma_tags = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    words = [word for tag in tagged_sentence for word in extract_words(tag)]\n",
    "    sentence = \" \".join(words)\n",
    "    for tag in tagged_sentence:\n",
    "        if isinstance(tag, Lemma):\n",
    "            lemma_tags.append(tag)\n",
    "            print(f\"Lemma tag: {tag}\")\n",
    "\n",
    "print(f\"Number of lemma tags: {len(lemma_tags)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mprint\u001b[39m(contexts[:\u001b[39m10\u001b[39m])\n\u001b[0;32m     36\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(contexts)\n\u001b[0;32m     38\u001b[0m y \u001b[39m=\u001b[39m senses\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.corpus import semcor\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Tree):\n",
    "        label = tag.label()\n",
    "        if isinstance(label, Synset):\n",
    "            return label.name()\n",
    "    return None\n",
    "\n",
    "def extract_words(tag):\n",
    "    if isinstance(tag, str):\n",
    "        return [tag]\n",
    "    elif isinstance(tag, Tree):\n",
    "        return [word for subtree in tag for word in extract_words(subtree)]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    words = [word for tag in tagged_sentence for word in extract_words(tag)]\n",
    "    sentence = \" \".join(words)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(contexts[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senses: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of senses: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(senses)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)  \u001b[39m# print the number of senses to see if we're getting any\u001b[39;00m\n\u001b[0;32m     37\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(contexts)\n\u001b[0;32m     39\u001b[0m y \u001b[39m=\u001b[39m senses\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader.wordnet import Synset\n",
    "from nltk.corpus import semcor\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_sense(tag):\n",
    "    if isinstance(tag, Tree):\n",
    "        label = tag.label()\n",
    "        if isinstance(label, Synset):\n",
    "            return label.name()\n",
    "    return None\n",
    "\n",
    "def extract_words(tag):\n",
    "    if isinstance(tag, str):\n",
    "        return [tag]\n",
    "    elif isinstance(tag, Tree):\n",
    "        return [word for subtree in tag for word in extract_words(subtree)]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    words = [word for tag in tagged_sentence for word in extract_words(tag)]\n",
    "    sentence = \" \".join(words)\n",
    "    for tag in tagged_sentence:\n",
    "        sense = get_sense(tag)\n",
    "        if sense is not None:\n",
    "            print(f\"Sense: {sense}\")  # print the sense to see if we're getting any\n",
    "            senses.append(sense)\n",
    "            contexts.append(sentence)\n",
    "\n",
    "print(f\"Number of senses: {len(senses)}\")  # print the number of senses to see if we're getting any\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mprint\u001b[39m(all_senses[:\u001b[39m10\u001b[39m])\n\u001b[0;32m     38\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(contexts)\n\u001b[0;32m     40\u001b[0m y \u001b[39m=\u001b[39m all_senses\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1294\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1292\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1293\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1294\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1295\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1296\u001b[0m         )\n\u001b[0;32m   1298\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "def extract_words_and_senses(tag):\n",
    "    if isinstance(tag, str):\n",
    "        return [tag], []\n",
    "    elif isinstance(tag, Tree):\n",
    "        words = []\n",
    "        senses = []\n",
    "        label = tag.label()\n",
    "        if isinstance(label, Synset):\n",
    "            senses.append(label.name())\n",
    "        for subtree in tag:\n",
    "            subwords, subsenses = extract_words_and_senses(subtree)\n",
    "            words.extend(subwords)\n",
    "            senses.extend(subsenses)\n",
    "        return words, senses\n",
    "    else:\n",
    "        return [], []\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='sem')\n",
    "\n",
    "contexts = []\n",
    "all_senses = []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence_words = []\n",
    "    sentence_senses = []\n",
    "    for tag in tagged_sentence:\n",
    "        words, senses = extract_words_and_senses(tag)\n",
    "        sentence_words.extend(words)\n",
    "        sentence_senses.extend(senses)\n",
    "    if sentence_senses:\n",
    "        context = \" \".join(sentence_words)\n",
    "        contexts.extend([context] * len(sentence_senses))\n",
    "        all_senses.extend(sentence_senses)\n",
    "\n",
    "print(contexts[:10])\n",
    "print(all_senses[:10])\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "X = vectorizer.fit_transform(contexts)\n",
    "y = all_senses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n",
      "['The', 'September', 'October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']\n",
      "['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.']\n",
      "['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', 'Georgia', \"'s\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']\n",
      "['It', 'recommended', 'that', 'Fulton', 'legislators', 'act', '``', 'to', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'to', 'the', 'end', 'of', 'modernizing', 'and', 'improving', 'them', \"''\", '.']\n",
      "['The', 'grand', 'jury', 'commented', 'on', 'a', 'number', 'of', 'other', 'topics', ',', 'among', 'them', 'the', 'Atlanta', 'and', 'Fulton', 'County', 'purchasing', 'departments', 'which', 'it', 'said', '``', 'are', 'well', 'operated', 'and', 'follow', 'generally', 'accepted', 'practices', 'which', 'inure', 'to', 'the', 'best', 'interest', 'of', 'both', 'governments', \"''\", '.']\n",
      "['However', ',', 'the', 'jury', 'said', 'it', 'believes', '``', 'these', 'two', 'offices', 'should', 'be', 'combined', 'to', 'achieve', 'greater', 'efficiency', 'and', 'reduce', 'the', 'cost', 'of', 'administration', \"''\", '.']\n",
      "['The', 'City', 'Purchasing', 'Department', ',', 'the', 'jury', 'said', ',', '``', 'is', 'lacking', 'in', 'experienced', 'clerical', 'personnel', 'as', 'a', 'result', 'of', 'city', 'personnel', 'policies', \"''\", '.']\n",
      "['It', 'urged', 'that', 'the', 'city', '``', 'take', 'steps', 'to', 'remedy', \"''\", 'this', 'problem', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in semcor.sents()[:10]:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']]\n",
      "[['The'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('far.r.02.far'), ['further']), Tree(Lemma('state.v.01.say'), ['said']), ['in'], Tree(Lemma('term.n.02.term'), ['term']), Tree(Lemma('end.n.02.end'), ['end']), Tree(Lemma('presentment.n.01.presentment'), ['presentments']), ['that'], ['the'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['City', 'Executive', 'Committee'])]), [','], ['which'], Tree(Lemma('own.v.01.have'), ['had']), Tree(Lemma('overall.s.02.overall'), ['over-all']), Tree(Lemma('mission.n.03.charge'), ['charge']), ['of'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), [','], ['``'], Tree(Lemma('deserve.v.01.deserve'), ['deserves']), ['the'], Tree(Lemma('praise.n.01.praise'), ['praise']), ['and'], Tree(Lemma('thanks.n.01.thanks'), ['thanks']), ['of'], ['the'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['City', 'of', 'Atlanta'])]), [\"''\"], ['for'], ['the'], Tree(Lemma('manner.n.01.manner'), ['manner']), ['in'], ['which'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), ['was'], Tree(Lemma('conduct.v.01.conduct'), ['conducted']), ['.']]\n",
      "[['The'], Tree(Lemma('september.n.01.September'), ['September']), Tree(Lemma('october.n.01.October'), ['October']), Tree(Lemma('term.n.02.term'), ['term']), Tree(Lemma('jury.n.01.jury'), ['jury']), ['had'], ['been'], Tree(Lemma('appoint.v.02.charge'), ['charged']), ['by'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['Fulton'])]), Tree(Lemma('person.n.01.person'), [Tree('NE', ['Superior', 'Court', 'Judge', 'Durwood', 'Pye'])]), ['to'], Tree(Lemma('investigate.v.02.investigate'), ['investigate']), Tree(Lemma('report.n.03.report'), ['reports']), ['of'], Tree(Lemma('potential.a.01.possible'), ['possible']), ['``'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), [\"''\"], ['in'], ['the'], Tree(Lemma('hard-fought.s.01.hard-fought'), ['hard-fought']), Tree(Lemma('primary.n.01.primary'), ['primary']), ['which'], ['was'], Tree(Lemma('win.v.01.win'), ['won']), ['by'], Tree(Lemma('person.n.01.person'), [Tree('NE', ['Mayor-nominate', 'Ivan', 'Allen', 'Jr.'])]), ['.']]\n",
      "[['``'], Tree(Lemma('merely.r.01.only'), ['Only']), ['a'], Tree(Lemma('relative.a.01.relative'), ['relative']), Tree(Lemma('handful.n.01.handful'), ['handful']), ['of'], Tree('such.s.00', ['such']), Tree(Lemma('report.n.03.report'), ['reports']), ['was'], Tree(Lemma('receive.v.02.receive'), ['received']), [\"''\"], [','], ['the'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('state.v.01.say'), ['said']), [','], ['``'], Tree(Lemma('consider.v.04.consider'), ['considering']), ['the'], Tree(Lemma('widespread.s.01.widespread'), ['widespread']), Tree(Lemma('interest.n.01.interest'), ['interest']), ['in'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), [','], ['the'], Tree(Lemma('number.n.02.number'), ['number']), ['of'], Tree(Lemma('voter.n.01.voter'), ['voters']), ['and'], ['the'], Tree(Lemma('size.n.01.size'), ['size']), ['of', 'this'], Tree(Lemma('city.n.01.city'), ['city']), [\"''\"], ['.']]\n",
      "[['The'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('allege.v.01.say'), ['said']), ['it'], ['did'], Tree(Lemma('rule.v.04.find'), ['find']), ['that'], ['many', 'of'], Tree(Lemma('georgia.n.01.Georgia'), ['Georgia']), [\"'s\"], Tree(Lemma('registration.n.01.registration'), ['registration']), ['and'], Tree(Lemma('election.n.01.election'), ['election']), Tree(Lemma('law.n.02.law'), ['laws']), ['``'], Tree(Lemma('be.v.01.be'), ['are']), Tree(Lemma('antique.s.02.outmoded'), ['outmoded']), ['or'], Tree(Lemma('inadequate.a.01.inadequate'), ['inadequate']), ['and'], Tree(Lemma('frequently.r.01.often'), ['often']), Tree(Lemma('ambiguous.a.02.ambiguous'), ['ambiguous']), [\"''\"], ['.']]\n",
      "[['It'], Tree(Lemma('recommend.v.01.recommend'), ['recommended']), ['that'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['Fulton'])]), Tree(Lemma('legislator.n.01.legislator'), ['legislators']), Tree(Lemma('act.v.01.act'), ['act']), ['``'], ['to'], Tree(Lemma('get.v.03.have'), ['have']), ['these'], Tree(Lemma('law.n.02.law'), ['laws']), Tree(Lemma('analyze.v.01.study'), ['studied']), ['and'], Tree(Lemma('revise.v.01.revise'), ['revised']), ['to'], ['the'], Tree(Lemma('goal.n.01.end'), ['end']), ['of'], Tree(Lemma('overhaul.v.02.modernize'), ['modernizing']), ['and'], Tree(Lemma('better.v.03.improve'), ['improving']), ['them'], [\"''\"], ['.']]\n",
      "[['The'], Tree(Lemma('grand_jury.n.01.grand_jury'), ['grand', 'jury']), Tree(Lemma('comment.v.01.comment'), ['commented']), ['on'], ['a'], Tree(Lemma('number.n.01.number'), ['number']), ['of'], Tree(Lemma('other.a.01.other'), ['other']), Tree(Lemma('topic.n.02.topic'), ['topics']), [','], ['among'], ['them'], ['the'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), ['and'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['Fulton', 'County'])]), Tree(Lemma('purchasing_department.n.01.purchasing_department'), ['purchasing', 'departments']), ['which'], ['it'], Tree(Lemma('allege.v.01.say'), ['said']), ['``'], Tree(Lemma('be.v.01.be'), ['are']), Tree(Lemma('well.r.01.well'), ['well']), Tree(Lemma('function.v.01.operate'), ['operated']), ['and'], Tree(Lemma('adopt.v.01.follow'), ['follow']), Tree(Lemma('by_and_large.r.01.generally'), ['generally']), Tree('accepted.s.00', ['accepted']), Tree(Lemma('practice.n.01.practice'), ['practices']), ['which'], Tree(Lemma('inure.v.01.inure'), ['inure']), ['to'], ['the'], Tree(Lemma('best.a.01.best'), ['best']), Tree(Lemma('sake.n.01.interest'), ['interest']), ['of'], ['both'], Tree(Lemma('government.n.01.government'), ['governments']), [\"''\"], ['.']]\n",
      "[Tree(Lemma('however.r.01.however'), ['However']), [','], ['the'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('allege.v.01.say'), ['said']), ['it'], Tree(Lemma('believe.v.03.believe'), ['believes']), ['``'], ['these'], Tree(Lemma('two.s.01.two'), ['two']), Tree(Lemma('agency.n.01.office'), ['offices']), ['should'], ['be'], Tree('combined.s.00', ['combined']), ['to'], Tree(Lemma('achieve.v.01.achieve'), ['achieve']), Tree('great.s.00', ['greater']), Tree(Lemma('efficiency.n.02.efficiency'), ['efficiency']), ['and'], Tree(Lemma('reduce.v.01.reduce'), ['reduce']), ['the'], Tree(Lemma('cost.n.01.cost'), ['cost']), ['of'], Tree(Lemma('administration.n.01.administration'), ['administration']), [\"''\"], ['.']]\n",
      "[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['City', 'Purchasing', 'Department'])]), [','], ['the'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('allege.v.01.say'), ['said']), [','], ['``'], Tree(Lemma('be.v.01.be'), ['is']), Tree(Lemma('deficient.s.01.lacking'), ['lacking']), ['in'], Tree(Lemma('experienced.a.01.experienced'), ['experienced']), Tree(Lemma('clerical.a.01.clerical'), ['clerical']), Tree(Lemma('force.n.04.personnel'), ['personnel']), ['as'], ['a'], Tree(Lemma('consequence.n.01.result'), ['result']), ['of'], Tree(Lemma('city.n.02.city'), ['city']), Tree(Lemma('force.n.04.personnel'), ['personnel']), Tree(Lemma('policy.n.01.policy'), ['policies']), [\"''\"], ['.']]\n",
      "[['It'], Tree(Lemma('urge.v.01.urge'), ['urged']), ['that'], ['the'], Tree(Lemma('city.n.02.city'), ['city']), ['``'], Tree(Lemma('take.v.01.take'), ['take']), Tree(Lemma('measure.n.01.step'), ['steps']), ['to'], Tree(Lemma('rectify.v.04.remedy'), ['remedy']), [\"''\"], ['this'], Tree(Lemma('problem.n.02.problem'), ['problem']), ['.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import semcor\n",
    "\n",
    "# Load the SemCor corpus\n",
    "corpus = semcor\n",
    "\n",
    "# Get the first 10 sense-tagged sentences\n",
    "tagged_sentences = corpus.tagged_sents(tag='sem')[:10]\n",
    "\n",
    "# Print the sense-tagged sentences\n",
    "for sentence in tagged_sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemma_pairs = [[(tree[0], tree.label()) if isinstance(tree, nltk.Tree) else (tree, None) for tree in sentence] for sentence in tagged_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(['The'], None), (Tree('NE', ['Fulton', 'County', 'Grand', 'Jury']), Lemma('group.n.01.group')), ('said', Lemma('state.v.01.say')), ('Friday', Lemma('friday.n.01.Friday')), (['an'], None), ('investigation', Lemma('probe.n.01.investigation')), (['of'], None), ('Atlanta', Lemma('atlanta.n.01.Atlanta')), ([\"'s\"], None), ('recent', Lemma('late.s.03.recent')), ('primary', Lemma('primary.n.01.primary_election')), ('produced', Lemma('produce.v.04.produce')), (['``'], None), (['no'], None), ('evidence', Lemma('evidence.n.01.evidence')), ([\"''\"], None), (['that'], None), (['any'], None), ('irregularities', Lemma('abnormality.n.04.irregularity')), ('took', Lemma('happen.v.01.take_place')), (['.'], None)], [(['The'], None), ('jury', Lemma('jury.n.01.jury')), ('further', Lemma('far.r.02.far')), ('said', Lemma('state.v.01.say')), (['in'], None), ('term', Lemma('term.n.02.term')), ('end', Lemma('end.n.02.end')), ('presentments', Lemma('presentment.n.01.presentment')), (['that'], None), (['the'], None), (Tree('NE', ['City', 'Executive', 'Committee']), Lemma('group.n.01.group')), ([','], None), (['which'], None), ('had', Lemma('own.v.01.have')), ('over-all', Lemma('overall.s.02.overall')), ('charge', Lemma('mission.n.03.charge')), (['of'], None), (['the'], None), ('election', Lemma('election.n.01.election')), ([','], None), (['``'], None), ('deserves', Lemma('deserve.v.01.deserve')), (['the'], None), ('praise', Lemma('praise.n.01.praise')), (['and'], None), ('thanks', Lemma('thanks.n.01.thanks')), (['of'], None), (['the'], None), (Tree('NE', ['City', 'of', 'Atlanta']), Lemma('location.n.01.location')), ([\"''\"], None), (['for'], None), (['the'], None), ('manner', Lemma('manner.n.01.manner')), (['in'], None), (['which'], None), (['the'], None), ('election', Lemma('election.n.01.election')), (['was'], None), ('conducted', Lemma('conduct.v.01.conduct')), (['.'], None)], [(['The'], None), ('September', Lemma('september.n.01.September')), ('October', Lemma('october.n.01.October')), ('term', Lemma('term.n.02.term')), ('jury', Lemma('jury.n.01.jury')), (['had'], None), (['been'], None), ('charged', Lemma('appoint.v.02.charge')), (['by'], None), (Tree('NE', ['Fulton']), Lemma('location.n.01.location')), (Tree('NE', ['Superior', 'Court', 'Judge', 'Durwood', 'Pye']), Lemma('person.n.01.person')), (['to'], None), ('investigate', Lemma('investigate.v.02.investigate')), ('reports', Lemma('report.n.03.report')), (['of'], None), ('possible', Lemma('potential.a.01.possible')), (['``'], None), ('irregularities', Lemma('abnormality.n.04.irregularity')), ([\"''\"], None), (['in'], None), (['the'], None), ('hard-fought', Lemma('hard-fought.s.01.hard-fought')), ('primary', Lemma('primary.n.01.primary')), (['which'], None), (['was'], None), ('won', Lemma('win.v.01.win')), (['by'], None), (Tree('NE', ['Mayor-nominate', 'Ivan', 'Allen', 'Jr.']), Lemma('person.n.01.person')), (['.'], None)], [(['``'], None), ('Only', Lemma('merely.r.01.only')), (['a'], None), ('relative', Lemma('relative.a.01.relative')), ('handful', Lemma('handful.n.01.handful')), (['of'], None), ('such', 'such.s.00'), ('reports', Lemma('report.n.03.report')), (['was'], None), ('received', Lemma('receive.v.02.receive')), ([\"''\"], None), ([','], None), (['the'], None), ('jury', Lemma('jury.n.01.jury')), ('said', Lemma('state.v.01.say')), ([','], None), (['``'], None), ('considering', Lemma('consider.v.04.consider')), (['the'], None), ('widespread', Lemma('widespread.s.01.widespread')), ('interest', Lemma('interest.n.01.interest')), (['in'], None), (['the'], None), ('election', Lemma('election.n.01.election')), ([','], None), (['the'], None), ('number', Lemma('number.n.02.number')), (['of'], None), ('voters', Lemma('voter.n.01.voter')), (['and'], None), (['the'], None), ('size', Lemma('size.n.01.size')), (['of', 'this'], None), ('city', Lemma('city.n.01.city')), ([\"''\"], None), (['.'], None)], [(['The'], None), ('jury', Lemma('jury.n.01.jury')), ('said', Lemma('allege.v.01.say')), (['it'], None), (['did'], None), ('find', Lemma('rule.v.04.find')), (['that'], None), (['many', 'of'], None), ('Georgia', Lemma('georgia.n.01.Georgia')), ([\"'s\"], None), ('registration', Lemma('registration.n.01.registration')), (['and'], None), ('election', Lemma('election.n.01.election')), ('laws', Lemma('law.n.02.law')), (['``'], None), ('are', Lemma('be.v.01.be')), ('outmoded', Lemma('antique.s.02.outmoded')), (['or'], None), ('inadequate', Lemma('inadequate.a.01.inadequate')), (['and'], None), ('often', Lemma('frequently.r.01.often')), ('ambiguous', Lemma('ambiguous.a.02.ambiguous')), ([\"''\"], None), (['.'], None)], [(['It'], None), ('recommended', Lemma('recommend.v.01.recommend')), (['that'], None), (Tree('NE', ['Fulton']), Lemma('location.n.01.location')), ('legislators', Lemma('legislator.n.01.legislator')), ('act', Lemma('act.v.01.act')), (['``'], None), (['to'], None), ('have', Lemma('get.v.03.have')), (['these'], None), ('laws', Lemma('law.n.02.law')), ('studied', Lemma('analyze.v.01.study')), (['and'], None), ('revised', Lemma('revise.v.01.revise')), (['to'], None), (['the'], None), ('end', Lemma('goal.n.01.end')), (['of'], None), ('modernizing', Lemma('overhaul.v.02.modernize')), (['and'], None), ('improving', Lemma('better.v.03.improve')), (['them'], None), ([\"''\"], None), (['.'], None)], [(['The'], None), ('grand', Lemma('grand_jury.n.01.grand_jury')), ('commented', Lemma('comment.v.01.comment')), (['on'], None), (['a'], None), ('number', Lemma('number.n.01.number')), (['of'], None), ('other', Lemma('other.a.01.other')), ('topics', Lemma('topic.n.02.topic')), ([','], None), (['among'], None), (['them'], None), (['the'], None), ('Atlanta', Lemma('atlanta.n.01.Atlanta')), (['and'], None), (Tree('NE', ['Fulton', 'County']), Lemma('location.n.01.location')), ('purchasing', Lemma('purchasing_department.n.01.purchasing_department')), (['which'], None), (['it'], None), ('said', Lemma('allege.v.01.say')), (['``'], None), ('are', Lemma('be.v.01.be')), ('well', Lemma('well.r.01.well')), ('operated', Lemma('function.v.01.operate')), (['and'], None), ('follow', Lemma('adopt.v.01.follow')), ('generally', Lemma('by_and_large.r.01.generally')), ('accepted', 'accepted.s.00'), ('practices', Lemma('practice.n.01.practice')), (['which'], None), ('inure', Lemma('inure.v.01.inure')), (['to'], None), (['the'], None), ('best', Lemma('best.a.01.best')), ('interest', Lemma('sake.n.01.interest')), (['of'], None), (['both'], None), ('governments', Lemma('government.n.01.government')), ([\"''\"], None), (['.'], None)], [('However', Lemma('however.r.01.however')), ([','], None), (['the'], None), ('jury', Lemma('jury.n.01.jury')), ('said', Lemma('allege.v.01.say')), (['it'], None), ('believes', Lemma('believe.v.03.believe')), (['``'], None), (['these'], None), ('two', Lemma('two.s.01.two')), ('offices', Lemma('agency.n.01.office')), (['should'], None), (['be'], None), ('combined', 'combined.s.00'), (['to'], None), ('achieve', Lemma('achieve.v.01.achieve')), ('greater', 'great.s.00'), ('efficiency', Lemma('efficiency.n.02.efficiency')), (['and'], None), ('reduce', Lemma('reduce.v.01.reduce')), (['the'], None), ('cost', Lemma('cost.n.01.cost')), (['of'], None), ('administration', Lemma('administration.n.01.administration')), ([\"''\"], None), (['.'], None)], [(['The'], None), (Tree('NE', ['City', 'Purchasing', 'Department']), Lemma('group.n.01.group')), ([','], None), (['the'], None), ('jury', Lemma('jury.n.01.jury')), ('said', Lemma('allege.v.01.say')), ([','], None), (['``'], None), ('is', Lemma('be.v.01.be')), ('lacking', Lemma('deficient.s.01.lacking')), (['in'], None), ('experienced', Lemma('experienced.a.01.experienced')), ('clerical', Lemma('clerical.a.01.clerical')), ('personnel', Lemma('force.n.04.personnel')), (['as'], None), (['a'], None), ('result', Lemma('consequence.n.01.result')), (['of'], None), ('city', Lemma('city.n.02.city')), ('personnel', Lemma('force.n.04.personnel')), ('policies', Lemma('policy.n.01.policy')), ([\"''\"], None), (['.'], None)], [(['It'], None), ('urged', Lemma('urge.v.01.urge')), (['that'], None), (['the'], None), ('city', Lemma('city.n.02.city')), (['``'], None), ('take', Lemma('take.v.01.take')), ('steps', Lemma('measure.n.01.step')), (['to'], None), ('remedy', Lemma('rectify.v.04.remedy')), ([\"''\"], None), (['this'], None), ('problem', Lemma('problem.n.02.problem')), (['.'], None)]]\n"
     ]
    }
   ],
   "source": [
    "print(word_lemma_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Tree('DT', ['The']), None), (['Fulton', 'County', 'Grand', 'Jury'], <bound method Lemma.synset of Lemma('group.n.01.group')>), (['said'], <bound method Lemma.synset of Lemma('state.v.01.say')>), (['Friday'], <bound method Lemma.synset of Lemma('friday.n.01.Friday')>), (Tree('DT', ['an']), None), (['investigation'], <bound method Lemma.synset of Lemma('probe.n.01.investigation')>), (Tree('IN', ['of']), None), (['Atlanta'], <bound method Lemma.synset of Lemma('atlanta.n.01.Atlanta')>), (Tree('POS', [\"'s\"]), None), (['recent'], <bound method Lemma.synset of Lemma('late.s.03.recent')>), (['primary', 'election'], <bound method Lemma.synset of Lemma('primary.n.01.primary_election')>), (['produced'], <bound method Lemma.synset of Lemma('produce.v.04.produce')>), (Tree(None, ['``']), None), (Tree('DT', ['no']), None), (['evidence'], <bound method Lemma.synset of Lemma('evidence.n.01.evidence')>), (Tree(None, [\"''\"]), None), (Tree('IN', ['that']), None), (Tree('DT', ['any']), None), (['irregularities'], <bound method Lemma.synset of Lemma('abnormality.n.04.irregularity')>), (['took', 'place'], <bound method Lemma.synset of Lemma('happen.v.01.take_place')>), (Tree(None, ['.']), None)]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = semcor.tagged_sents(tag='both')\n",
    "\n",
    "word_lemma_pairs = [[(tree.leaves(), tree.label().synset) if isinstance(tree, nltk.Tree) and isinstance(tree.label(), nltk.corpus.reader.wordnet.Lemma) else (tree, None) for tree in sentence] for sentence in tagged_sentences]\n",
    "\n",
    "print(word_lemma_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Tree('DT', ['The']), None), (['Fulton', 'County', 'Grand', 'Jury'], Synset('group.n.01')), (['said'], Synset('state.v.01')), (['Friday'], Synset('friday.n.01')), (Tree('DT', ['an']), None), (['investigation'], Synset('probe.n.01')), (Tree('IN', ['of']), None), (['Atlanta'], Synset('atlanta.n.01')), (Tree('POS', [\"'s\"]), None), (['recent'], Synset('late.s.03')), (['primary', 'election'], Synset('primary.n.01')), (['produced'], Synset('produce.v.04')), (Tree(None, ['``']), None), (Tree('DT', ['no']), None), (['evidence'], Synset('evidence.n.01')), (Tree(None, [\"''\"]), None), (Tree('IN', ['that']), None), (Tree('DT', ['any']), None), (['irregularities'], Synset('abnormality.n.04')), (['took', 'place'], Synset('happen.v.01')), (Tree(None, ['.']), None)]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = semcor.tagged_sents(tag='both')\n",
    "\n",
    "word_lemma_pairs = [[(tree.leaves(), tree.label().synset()) if isinstance(tree, nltk.Tree) and isinstance(tree.label(), nltk.corpus.reader.wordnet.Lemma) else (tree, None) for tree in sentence] for sentence in tagged_sentences]\n",
    "\n",
    "print(word_lemma_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Assuming word_lemma_pairs is your data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X \u001b[39m=\u001b[39m [x[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m word_lemma_pairs]  \u001b[39m# input data (context words)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m word_lemma_pairs]  \u001b[39m# target data (word senses)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Assuming word_lemma_pairs is your data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m X \u001b[39m=\u001b[39m [x[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m word_lemma_pairs]  \u001b[39m# input data (context words)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y \u001b[39m=\u001b[39m [x[\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m word_lemma_pairs]  \u001b[39m# target data (word senses)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming word_lemma_pairs is your data\n",
    "X = [x[0] for x in word_lemma_pairs]  # input data (context words)\n",
    "y = [x[1] for x in word_lemma_pairs]  # target data (word senses)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(['The'], None), (['Fulton', 'County', 'Grand', 'Jury'], Lemma('group.n.01.group')), (['said'], Lemma('state.v.01.say')), (['Friday'], Lemma('friday.n.01.Friday')), (['an'], None), (['investigation'], Lemma('probe.n.01.investigation')), (['of'], None), (['Atlanta'], Lemma('atlanta.n.01.Atlanta')), ([\"'s\"], None), (['recent'], Lemma('late.s.03.recent')), (['primary', 'election'], Lemma('primary.n.01.primary_election')), (['produced'], Lemma('produce.v.04.produce')), (['``'], None), (['no'], None), (['evidence'], Lemma('evidence.n.01.evidence')), ([\"''\"], None), (['that'], None), (['any'], None), (['irregularities'], Lemma('abnormality.n.04.irregularity')), (['took', 'place'], Lemma('happen.v.01.take_place')), (['.'], None)], [(['The'], None), (['jury'], Lemma('jury.n.01.jury')), (['further'], Lemma('far.r.02.far')), (['said'], Lemma('state.v.01.say')), (['in'], None), (['term'], Lemma('term.n.02.term')), (['end'], Lemma('end.n.02.end')), (['presentments'], Lemma('presentment.n.01.presentment')), (['that'], None), (['the'], None), (['City', 'Executive', 'Committee'], Lemma('group.n.01.group')), ([','], None), (['which'], None), (['had'], Lemma('own.v.01.have')), (['over-all'], Lemma('overall.s.02.overall')), (['charge'], Lemma('mission.n.03.charge')), (['of'], None), (['the'], None), (['election'], Lemma('election.n.01.election')), ([','], None), (['``'], None), (['deserves'], Lemma('deserve.v.01.deserve')), (['the'], None), (['praise'], Lemma('praise.n.01.praise')), (['and'], None), (['thanks'], Lemma('thanks.n.01.thanks')), (['of'], None), (['the'], None), (['City', 'of', 'Atlanta'], Lemma('location.n.01.location')), ([\"''\"], None), (['for'], None), (['the'], None), (['manner'], Lemma('manner.n.01.manner')), (['in'], None), (['which'], None), (['the'], None), (['election'], Lemma('election.n.01.election')), (['was'], None), (['conducted'], Lemma('conduct.v.01.conduct')), (['.'], None)], [(['The'], None), (['September'], Lemma('september.n.01.September')), (['October'], Lemma('october.n.01.October')), (['term'], Lemma('term.n.02.term')), (['jury'], Lemma('jury.n.01.jury')), (['had'], None), (['been'], None), (['charged'], Lemma('appoint.v.02.charge')), (['by'], None), (['Fulton'], Lemma('location.n.01.location')), (['Superior', 'Court', 'Judge', 'Durwood', 'Pye'], Lemma('person.n.01.person')), (['to'], None), (['investigate'], Lemma('investigate.v.02.investigate')), (['reports'], Lemma('report.n.03.report')), (['of'], None), (['possible'], Lemma('potential.a.01.possible')), (['``'], None), (['irregularities'], Lemma('abnormality.n.04.irregularity')), ([\"''\"], None), (['in'], None), (['the'], None), (['hard-fought'], Lemma('hard-fought.s.01.hard-fought')), (['primary'], Lemma('primary.n.01.primary')), (['which'], None), (['was'], None), (['won'], Lemma('win.v.01.win')), (['by'], None), (['Mayor-nominate', 'Ivan', 'Allen', 'Jr.'], Lemma('person.n.01.person')), (['.'], None)]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import semcor\n",
    "import nltk\n",
    "\n",
    "tagged_sentences = semcor.tagged_sents(tag='both')\n",
    "\n",
    "word_lemma_pairs = [[(tree.leaves(), tree.label() if isinstance(tree.label(), nltk.corpus.reader.wordnet.Lemma) else None) \n",
    "                      for tree in sentence] \n",
    "                     for sentence in tagged_sentences]\n",
    "\n",
    "print(word_lemma_pairs[:3])  # print the first 3 sentences to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([None, None, ['Fulton', 'County', 'Grand', 'Jury'], ['said']],\n",
       "  ['The'],\n",
       "  None),\n",
       " ([None, ['The'], ['said'], ['Friday']],\n",
       "  ['Fulton', 'County', 'Grand', 'Jury'],\n",
       "  Lemma('group.n.01.group')),\n",
       " ([['The'], ['Fulton', 'County', 'Grand', 'Jury'], ['Friday'], ['an']],\n",
       "  ['said'],\n",
       "  Lemma('state.v.01.say')),\n",
       " ([['Fulton', 'County', 'Grand', 'Jury'], ['said'], ['an'], ['investigation']],\n",
       "  ['Friday'],\n",
       "  Lemma('friday.n.01.Friday')),\n",
       " ([['said'], ['Friday'], ['investigation'], ['of']], ['an'], None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_context_window(word_lemma_pairs, window_size=2):\n",
    "    \"\"\"\n",
    "    Create context windows for each word in each sentence.\n",
    "\n",
    "    Each data point will consist of a word, its sense, and the words \n",
    "    in its context window. If a word is near the beginning or end of \n",
    "    a sentence, the context window will be padded with None.\n",
    "\n",
    "    Args:\n",
    "        word_lemma_pairs: List of sentences, where each sentence is a\n",
    "            list of (word, lemma) pairs.\n",
    "        window_size: The number of words to include in the context\n",
    "            window on each side of the target word.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples, where each tuple is (context, word, lemma).\n",
    "        context is a list of words, word is a string, and lemma is a\n",
    "        Synset object or None.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for sentence in word_lemma_pairs:\n",
    "        # Pad the sentence with None for context windows at the start and end\n",
    "        padded_sentence = [(None, None)]*window_size + sentence + [(None, None)]*window_size\n",
    "        for i in range(window_size, len(padded_sentence) - window_size):\n",
    "            context = [padded_sentence[i+j][0] for j in range(-window_size, window_size+1) if j != 0]\n",
    "            word, lemma = padded_sentence[i]\n",
    "            data.append((context, word, lemma))\n",
    "    return data\n",
    "\n",
    "data = create_context_window(word_lemma_pairs)\n",
    "data[:5]  # show the first 5 data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming \"data\" is your list of (context, word, lemma) tuples\n",
    "X = [x[0] for x in data]  # input data (context words)\n",
    "y = [x[2] for x in data]  # target data (word senses)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the training data and transform the training data\n",
    "# Transform the testing data\n",
    "X_train_vect = vectorizer.fit_transform([' '.join(word[0] if isinstance(word, list) else '' for word in context) for context in X_train])\n",
    "X_test_vect = vectorizer.transform([' '.join(word[0] if isinstance(word, list) else '' for word in context) for context in X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([Lemma('goat.n.01.goat'), None, None, ...,\n       Lemma('first.a.01.first'), None, None], dtype=object),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m clf \u001b[39m=\u001b[39m MultinomialNB()\n\u001b[0;32m      6\u001b[0m \u001b[39m# Train the classifier\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train_vect, y_train)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\naive_bayes.py:753\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    750\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    752\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n\u001b[1;32m--> 753\u001b[0m Y \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39;49mfit_transform(y)\n\u001b[0;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39mclasses_\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:334\u001b[0m, in \u001b[0;36mLabelBinarizer.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m    315\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[39m    The output of transform is sometimes referred to as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39m        will be of CSR format.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(y)\u001b[39m.\u001b[39mtransform(y)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:311\u001b[0m, in \u001b[0;36mLabelBinarizer.fit\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my has 0 samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y)\n\u001b[0;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse_input_ \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39missparse(y)\n\u001b[1;32m--> 311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m unique_labels(y)\n\u001b[0;32m    312\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\multiclass.py:107\u001b[0m, in \u001b[0;36munique_labels\u001b[1;34m(*ys)\u001b[0m\n\u001b[0;32m    105\u001b[0m _unique_labels \u001b[39m=\u001b[39m _FN_UNIQUE_LABELS\u001b[39m.\u001b[39mget(label_type, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _unique_labels:\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mrepr\u001b[39m(ys))\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m is_array_api:\n\u001b[0;32m    110\u001b[0m     \u001b[39m# array_api does not allow for mixed dtypes\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     unique_ys \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mconcat([_unique_labels(y) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m ys])\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: (array([Lemma('goat.n.01.goat'), None, None, ...,\n       Lemma('first.a.01.first'), None, None], dtype=object),)"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 140. GiB for an array with shape (622869, 30126) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m y_test_str \u001b[39m=\u001b[39m [\u001b[39mstr\u001b[39m(label) \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m y_test]\n\u001b[0;32m      5\u001b[0m \u001b[39m# Train the classifier again\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train_vect, y_train_str)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\naive_bayes.py:753\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    750\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    752\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n\u001b[1;32m--> 753\u001b[0m Y \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39;49mfit_transform(y)\n\u001b[0;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39mclasses_\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:334\u001b[0m, in \u001b[0;36mLabelBinarizer.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m    315\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[39m    The output of transform is sometimes referred to as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39m        will be of CSR format.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(y)\u001b[39m.\u001b[39;49mtransform(y)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:362\u001b[0m, in \u001b[0;36mLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39mif\u001b[39;00m y_is_multilabel \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    360\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe object was not fitted with multilabel input.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m label_binarize(\n\u001b[0;32m    363\u001b[0m     y,\n\u001b[0;32m    364\u001b[0m     classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_,\n\u001b[0;32m    365\u001b[0m     pos_label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_label,\n\u001b[0;32m    366\u001b[0m     neg_label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneg_label,\n\u001b[0;32m    367\u001b[0m     sparse_output\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse_output,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:571\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[1;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m target data is not supported with label binarization\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sparse_output:\n\u001b[1;32m--> 571\u001b[0m     Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m    572\u001b[0m     Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    574\u001b[0m     \u001b[39mif\u001b[39;00m neg_label \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 140. GiB for an array with shape (622869, 30126) and data type int64"
     ]
    }
   ],
   "source": [
    "# Convert labels to strings\n",
    "y_train_str = [str(label) for label in y_train]\n",
    "y_test_str = [str(label) for label in y_test]\n",
    "\n",
    "# Train the classifier again\n",
    "clf.fit(X_train_vect, y_train_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:163: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m SGDClassifier\n\u001b[0;32m      3\u001b[0m clf \u001b[39m=\u001b[39m SGDClassifier(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train_vect, y_train_str)\n\u001b[0;32m      6\u001b[0m y_pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test_vect)\n\u001b[0;32m      8\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(y_test_str, y_pred)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:894\u001b[0m, in \u001b[0;36mBaseSGDClassifier.fit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    892\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_more_validate_params()\n\u001b[1;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m    895\u001b[0m     X,\n\u001b[0;32m    896\u001b[0m     y,\n\u001b[0;32m    897\u001b[0m     alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[0;32m    898\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[0;32m    899\u001b[0m     loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[0;32m    900\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[0;32m    901\u001b[0m     coef_init\u001b[39m=\u001b[39;49mcoef_init,\n\u001b[0;32m    902\u001b[0m     intercept_init\u001b[39m=\u001b[39;49mintercept_init,\n\u001b[0;32m    903\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    904\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:683\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[39m# Clear iteration count for multiple call to fit.\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_ \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m--> 683\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[0;32m    684\u001b[0m     X,\n\u001b[0;32m    685\u001b[0m     y,\n\u001b[0;32m    686\u001b[0m     alpha,\n\u001b[0;32m    687\u001b[0m     C,\n\u001b[0;32m    688\u001b[0m     loss,\n\u001b[0;32m    689\u001b[0m     learning_rate,\n\u001b[0;32m    690\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    691\u001b[0m     classes,\n\u001b[0;32m    692\u001b[0m     sample_weight,\n\u001b[0;32m    693\u001b[0m     coef_init,\n\u001b[0;32m    694\u001b[0m     intercept_init,\n\u001b[0;32m    695\u001b[0m )\n\u001b[0;32m    697\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39m>\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[0;32m    700\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\n\u001b[0;32m    701\u001b[0m ):\n\u001b[0;32m    702\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMaximum number of iteration reached before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconvergence. Consider increasing max_iter to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    705\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimprove the fit.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    706\u001b[0m         ConvergenceWarning,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:617\u001b[0m, in \u001b[0;36mBaseSGDClassifier._partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[39m# delegate to concrete training procedure\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[39mif\u001b[39;00m n_classes \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 617\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_multiclass(\n\u001b[0;32m    618\u001b[0m         X,\n\u001b[0;32m    619\u001b[0m         y,\n\u001b[0;32m    620\u001b[0m         alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[0;32m    621\u001b[0m         C\u001b[39m=\u001b[39;49mC,\n\u001b[0;32m    622\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[0;32m    623\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    624\u001b[0m         max_iter\u001b[39m=\u001b[39;49mmax_iter,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[0;32m    626\u001b[0m \u001b[39melif\u001b[39;00m n_classes \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_binary(\n\u001b[0;32m    628\u001b[0m         X,\n\u001b[0;32m    629\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m         max_iter\u001b[39m=\u001b[39mmax_iter,\n\u001b[0;32m    635\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:760\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit_multiclass\u001b[1;34m(self, X, y, alpha, C, learning_rate, sample_weight, max_iter)\u001b[0m\n\u001b[0;32m    758\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[0;32m    759\u001b[0m seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_))\n\u001b[1;32m--> 760\u001b[0m result \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    761\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    762\u001b[0m )(\n\u001b[0;32m    763\u001b[0m     delayed(fit_binary)(\n\u001b[0;32m    764\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    765\u001b[0m         i,\n\u001b[0;32m    766\u001b[0m         X,\n\u001b[0;32m    767\u001b[0m         y,\n\u001b[0;32m    768\u001b[0m         alpha,\n\u001b[0;32m    769\u001b[0m         C,\n\u001b[0;32m    770\u001b[0m         learning_rate,\n\u001b[0;32m    771\u001b[0m         max_iter,\n\u001b[0;32m    772\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_expanded_class_weight[i],\n\u001b[0;32m    773\u001b[0m         \u001b[39m1.0\u001b[39;49m,\n\u001b[0;32m    774\u001b[0m         sample_weight,\n\u001b[0;32m    775\u001b[0m         validation_mask\u001b[39m=\u001b[39;49mvalidation_mask,\n\u001b[0;32m    776\u001b[0m         random_state\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, seed \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(seeds)\n\u001b[0;32m    779\u001b[0m )\n\u001b[0;32m    781\u001b[0m \u001b[39m# take the maximum of n_iter_ over every binary fit\u001b[39;00m\n\u001b[0;32m    782\u001b[0m n_iter_ \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:446\u001b[0m, in \u001b[0;36mfit_binary\u001b[1;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001b[0m\n\u001b[0;32m    442\u001b[0m seed \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT)\n\u001b[0;32m    444\u001b[0m tol \u001b[39m=\u001b[39m est\u001b[39m.\u001b[39mtol \u001b[39mif\u001b[39;00m est\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[1;32m--> 446\u001b[0m coef, intercept, average_coef, average_intercept, n_iter_ \u001b[39m=\u001b[39m _plain_sgd(\n\u001b[0;32m    447\u001b[0m     coef,\n\u001b[0;32m    448\u001b[0m     intercept,\n\u001b[0;32m    449\u001b[0m     average_coef,\n\u001b[0;32m    450\u001b[0m     average_intercept,\n\u001b[0;32m    451\u001b[0m     est\u001b[39m.\u001b[39;49mloss_function_,\n\u001b[0;32m    452\u001b[0m     penalty_type,\n\u001b[0;32m    453\u001b[0m     alpha,\n\u001b[0;32m    454\u001b[0m     C,\n\u001b[0;32m    455\u001b[0m     est\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[0;32m    456\u001b[0m     dataset,\n\u001b[0;32m    457\u001b[0m     validation_mask,\n\u001b[0;32m    458\u001b[0m     est\u001b[39m.\u001b[39;49mearly_stopping,\n\u001b[0;32m    459\u001b[0m     validation_score_cb,\n\u001b[0;32m    460\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mn_iter_no_change),\n\u001b[0;32m    461\u001b[0m     max_iter,\n\u001b[0;32m    462\u001b[0m     tol,\n\u001b[0;32m    463\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mfit_intercept),\n\u001b[0;32m    464\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mverbose),\n\u001b[0;32m    465\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mshuffle),\n\u001b[0;32m    466\u001b[0m     seed,\n\u001b[0;32m    467\u001b[0m     pos_weight,\n\u001b[0;32m    468\u001b[0m     neg_weight,\n\u001b[0;32m    469\u001b[0m     learning_rate_type,\n\u001b[0;32m    470\u001b[0m     est\u001b[39m.\u001b[39;49meta0,\n\u001b[0;32m    471\u001b[0m     est\u001b[39m.\u001b[39;49mpower_t,\n\u001b[0;32m    472\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    473\u001b[0m     est\u001b[39m.\u001b[39;49mt_,\n\u001b[0;32m    474\u001b[0m     intercept_decay,\n\u001b[0;32m    475\u001b[0m     est\u001b[39m.\u001b[39;49maverage,\n\u001b[0;32m    476\u001b[0m )\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m est\u001b[39m.\u001b[39maverage:\n\u001b[0;32m    479\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(est\u001b[39m.\u001b[39mclasses_) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='log')\n",
    "clf.fit(X_train_vect, y_train_str)\n",
    "\n",
    "y_pred = clf.predict(X_test_vect)\n",
    "\n",
    "accuracy = accuracy_score(y_test_str, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 140. GiB for an array with shape (622869, 30126) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnaive_bayes\u001b[39;00m \u001b[39mimport\u001b[39;00m BernoulliNB\n\u001b[0;32m      3\u001b[0m clf \u001b[39m=\u001b[39m BernoulliNB()\n\u001b[1;32m----> 4\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train_vect, y_train_str)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\naive_bayes.py:753\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    750\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    752\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n\u001b[1;32m--> 753\u001b[0m Y \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39;49mfit_transform(y)\n\u001b[0;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39mclasses_\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:334\u001b[0m, in \u001b[0;36mLabelBinarizer.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m    315\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[39m    The output of transform is sometimes referred to as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39m        will be of CSR format.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(y)\u001b[39m.\u001b[39;49mtransform(y)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:362\u001b[0m, in \u001b[0;36mLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39mif\u001b[39;00m y_is_multilabel \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    360\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe object was not fitted with multilabel input.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m label_binarize(\n\u001b[0;32m    363\u001b[0m     y,\n\u001b[0;32m    364\u001b[0m     classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_,\n\u001b[0;32m    365\u001b[0m     pos_label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_label,\n\u001b[0;32m    366\u001b[0m     neg_label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneg_label,\n\u001b[0;32m    367\u001b[0m     sparse_output\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse_output,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:571\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[1;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m target data is not supported with label binarization\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sparse_output:\n\u001b[1;32m--> 571\u001b[0m     Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m    572\u001b[0m     Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    574\u001b[0m     \u001b[39mif\u001b[39;00m neg_label \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 140. GiB for an array with shape (622869, 30126) and data type int64"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_vect, y_train_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m combined_data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(X_train, y_train_str))\n\u001b[0;32m      4\u001b[0m \u001b[39m# Then, let's shuffle the data to ensure a random sample\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m random\u001b[39m.\u001b[39mshuffle(combined_data)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Now, let's take a smaller sample of the data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m sample_size \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m  \u001b[39m# adjust this value based on the memory constraints of your machine\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# First, let's combine the training data and labels into a single list of tuples\n",
    "combined_data = list(zip(X_train, y_train_str))\n",
    "\n",
    "# Then, let's shuffle the data to ensure a random sample\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "# Now, let's take a smaller sample of the data\n",
    "sample_size = 10000  # adjust this value based on the memory constraints of your machine\n",
    "smaller_sample = combined_data[:sample_size]\n",
    "\n",
    "# Finally, let's split the smaller sample back into training data and labels\n",
    "X_train_small = [x[0] for x in smaller_sample]\n",
    "y_train_small_str = [x[1] for x in smaller_sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.76 GiB for an array with shape (62286, 8103) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnaive_bayes\u001b[39;00m \u001b[39mimport\u001b[39;00m BernoulliNB\n\u001b[0;32m     14\u001b[0m clf \u001b[39m=\u001b[39m BernoulliNB()\n\u001b[1;32m---> 15\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train_sample, y_train_sample_str)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\naive_bayes.py:753\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    750\u001b[0m _, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    752\u001b[0m labelbin \u001b[39m=\u001b[39m LabelBinarizer()\n\u001b[1;32m--> 753\u001b[0m Y \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39;49mfit_transform(y)\n\u001b[0;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m labelbin\u001b[39m.\u001b[39mclasses_\n\u001b[0;32m    755\u001b[0m \u001b[39mif\u001b[39;00m Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:334\u001b[0m, in \u001b[0;36mLabelBinarizer.fit_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m    315\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit label binarizer/transform multi-class labels to binary labels.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[39m    The output of transform is sometimes referred to as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39m        will be of CSR format.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(y)\u001b[39m.\u001b[39;49mtransform(y)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:362\u001b[0m, in \u001b[0;36mLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39mif\u001b[39;00m y_is_multilabel \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    360\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe object was not fitted with multilabel input.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 362\u001b[0m \u001b[39mreturn\u001b[39;00m label_binarize(\n\u001b[0;32m    363\u001b[0m     y,\n\u001b[0;32m    364\u001b[0m     classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_,\n\u001b[0;32m    365\u001b[0m     pos_label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_label,\n\u001b[0;32m    366\u001b[0m     neg_label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneg_label,\n\u001b[0;32m    367\u001b[0m     sparse_output\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse_output,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:571\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[1;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m target data is not supported with label binarization\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type\n\u001b[0;32m    568\u001b[0m     )\n\u001b[0;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sparse_output:\n\u001b[1;32m--> 571\u001b[0m     Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39;49mtoarray()\n\u001b[0;32m    572\u001b[0m     Y \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    574\u001b[0m     \u001b[39mif\u001b[39;00m neg_label \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\melody\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.76 GiB for an array with shape (62286, 8103) and data type int64"
     ]
    }
   ],
   "source": [
    "# Determine the number of samples to take\n",
    "sample_size = int(X_train_vect.shape[0] * 0.10)\n",
    "\n",
    "# Slice the training data\n",
    "X_train_sample = X_train_vect[:sample_size]\n",
    "y_train_sample = y_train[:sample_size]\n",
    "\n",
    "# Convert lemmas to strings\n",
    "y_train_sample_str = [str(lemma) for lemma in y_train_sample]\n",
    "\n",
    "# Now you can try fitting the Naive Bayes model again\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_sample, y_train_sample_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of samples to take\n",
    "sample_size = int(X_train_vect.shape[0] * 0.01)  # use 0.005 for 0.5%\n",
    "\n",
    "# Slice the training data\n",
    "X_train_sample = X_train_vect[:sample_size]\n",
    "y_train_sample = y_train[:sample_size]\n",
    "\n",
    "# Convert lemmas to strings\n",
    "y_train_sample_str = [str(lemma) for lemma in y_train_sample]\n",
    "\n",
    "# Now you can try fitting the Naive Bayes model again\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train_sample, y_train_sample_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_str = [str(lemma) for lemma in y_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7110867080234783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test_str, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of samples to take\n",
    "sample_size = int(X_train_vect.shape[0] * 0.02)  # use 0.005 for 0.5%\n",
    "\n",
    "# Slice the training data\n",
    "X_train_sample = X_train_vect[:sample_size]\n",
    "y_train_sample = y_train[:sample_size]\n",
    "\n",
    "# Convert lemmas to strings\n",
    "y_train_sample_str = [str(lemma) for lemma in y_train_sample]\n",
    "\n",
    "# Now you can try fitting the Naive Bayes model again\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_sample, y_train_sample_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_str = [str(lemma) for lemma in y_test]\n",
    "y_pred = clf.predict(X_test_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7110867080234783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test_str, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by vectorizing the new data in the same way as you did for the training data, and then calling the predict method of the classifier.\n",
    "\n",
    "the BernoulliNB model you trained is not a word sense disambiguation model in the same sense as the Lesk algorithm is. Your model is trained to predict word senses based on the bag-of-words representation of the context in which a word appears, not based on the word itself. Therefore, the predictions of your model are not tied to a specific ambiguous word, but rather to the context. This is different from the Lesk algorithm, which predicts the sense of a specific word given its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying issue is that Naive Bayes classifiers are not typically amenable to mini-batch learning because they are not iterative models. The Naive Bayes algorithm calculates probabilities based on the entire dataset at once, rather than iteratively refining the model parameters like gradient descent-based models do. This makes it difficult to implement mini-batch learning for Naive Bayes.\n",
    "\n",
    "There are a few possible workarounds to handle large datasets with Naive Bayes:\n",
    "\n",
    "Feature Selection/Reduction: Reduce the number of features in your data. You might be able to reduce the dimensionality of your data using techniques like PCA (Principal Component Analysis) or by manually selecting a subset of your features.\n",
    "\n",
    "Data Sampling: Sample a subset of your data for the training process. While this will not use all your data, if your dataset is large and representative enough, this might provide sufficient results.\n",
    "\n",
    "Distributed Computing: Use distributed computing frameworks like Apache Spark's MLlib, which includes a distributed implementation of Naive Bayes.\n",
    "\n",
    "Use Other Libraries: Some libraries might offer Naive Bayes classifiers that can handle larger datasets or support mini-batch learning. One such library could be vowpal wabbit, but it would require a different syntax and possibly a different way to preprocess your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted sense for 'bank' is: None\n",
      "The predicted sense for 'bank' is: None\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "# Suppose these are your new sentences and ambiguous words\n",
    "new_sentences = [\"I went to the bank to deposit my money\", \"The river bank was very steep\"]\n",
    "ambiguous_words = [\"bank\", \"bank\"]\n",
    "\n",
    "# Tokenize the sentences to get the context for each word\n",
    "new_contexts = [word_tokenize(s) for s in new_sentences]\n",
    "\n",
    "# Vectorize the new data\n",
    "new_X_vect = vectorizer.transform([' '.join(word[0] if isinstance(word, list) else '' for word in context) for context in new_contexts])\n",
    "\n",
    "# Predict the senses for the new data\n",
    "new_y_pred = clf.predict(new_X_vect)\n",
    "\n",
    "# Print the predicted senses\n",
    "for word, pred in zip(ambiguous_words, new_y_pred):\n",
    "    print(f\"The predicted sense for '{word}' is: {pred}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the Lesk algorithm is not typically used in a machine learning setting, and it does not require a training phase like the Naive Bayes classifier does. The Lesk algorithm is a rule-based method that makes its decisions based on dictionary definitions and does not learn from data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a container (usually with a slot in the top) for keeping money at home\n",
      "put into a bank account\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Prepare the data\n",
    "sentences = [\"I went to the bank to deposit my money\", \"The river bank was very steep\"]\n",
    "ambiguous_words = [\"bank\", \"bank\"]\n",
    "contexts = [word_tokenize(s) for s in sentences]\n",
    "\n",
    "# Disambiguate the words\n",
    "for context, ambiguous_word in zip(contexts, ambiguous_words):\n",
    "    print(lesk(context, ambiguous_word).definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vowpalwabbit\n",
      "  Downloading vowpalwabbit-9.8.0-cp39-cp39-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 9.3 MB/s eta 0:00:00\n",
      "Installing collected packages: vowpalwabbit\n",
      "Successfully installed vowpalwabbit-9.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install vowpalwabbit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\L5 15IMH6-58IX\\AppData\\Local\\Temp\\ipykernel_21940\\4265257350.py:4: DeprecationWarning: vw has been renamed to Workspace, the alias will be removed in the future\n",
      "  vw = pyvw.vw(\"--binary\")\n"
     ]
    }
   ],
   "source": [
    "from vowpalwabbit import pyvw\n",
    "\n",
    "# Initialize model\n",
    "vw = pyvw.vw(\"--binary\")\n",
    "\n",
    "# Train model\n",
    "for example in [\"-1 | feature1:2.5 feature2:-1.5\",\n",
    "                \"1 | feature1:0.3 feature2:1.5\",\n",
    "                \"-1 | feature1:-1.0 feature2:0.2\"]:\n",
    "    vw.learn(example)\n",
    "\n",
    "# Predict\n",
    "ex = \"| feature1:1.0 feature2:0.5\"\n",
    "prediction = vw.predict(ex)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stops here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('path_to_your_file.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for child in root:\n",
    "    print(child.tag, child.attrib)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melody",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
